{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install librosa\n",
    "!pip install datasets[audio]\n",
    "!pip install gradio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "\n",
    "array, sampling_rate = librosa.load(librosa.ex(\"trumpet\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.waveshow(array, sr=sampling_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "dft_input = array[:4096]\n",
    "\n",
    "# calculate the DFT\n",
    "window = np.hanning(len(dft_input))\n",
    "windowed_input = dft_input * window\n",
    "dft = np.fft.rfft(windowed_input)\n",
    "\n",
    "# get the amplitude spectrum in decibels\n",
    "amplitude = np.abs(dft)\n",
    "amplitude_db = librosa.amplitude_to_db(amplitude, ref=np.max)\n",
    "\n",
    "# get the frequency bins\n",
    "frequency = librosa.fft_frequencies(sr=sampling_rate, n_fft=len(dft_input))\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "plt.plot(frequency, amplitude_db)\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"Amplitude (dB)\")\n",
    "plt.xscale(\"log\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "D = librosa.stft(array)\n",
    "S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(S_db, x_axis=\"time\", y_axis=\"hz\")\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = librosa.feature.melspectrogram(\n",
    "    y=array, sr=sampling_rate, n_mels=128, fmax=8000\n",
    ")\n",
    "S_dB = librosa.power_to_db(S, ref=np.max)\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(\n",
    "    S_dB, x_axis=\"time\", y_axis=\"mel\", sr=sampling_rate, fmax=8000\n",
    ")\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "minds = load_dataset(\"PolyAI/minds14\", name=\"en-AU\", split=\"train\")\n",
    "minds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = minds[0]\n",
    "example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = minds.features[\"intent_class\"].int2str\n",
    "id2label(example[\"intent_class\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_remove = [\"lang_id\", \"english_transcription\"]\n",
    "minds = minds.remove_columns(columns_to_remove)\n",
    "minds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr\n",
    "\n",
    "\n",
    "def generate_audio():\n",
    "    example = minds.shuffle()[0]\n",
    "    audio = example[\"audio\"]\n",
    "    return (\n",
    "        audio[\"sampling_rate\"],\n",
    "        audio[\"array\"],\n",
    "    ), id2label(example[\"intent_class\"])\n",
    "\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    with gr.Column():\n",
    "        for _ in range(4):\n",
    "            audio, label = generate_audio()\n",
    "            output = gr.Audio(audio, label=label)\n",
    "\n",
    "demo.launch(debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import matplotlib.pyplot as plt\n",
    "import librosa.display\n",
    "\n",
    "array = example[\"audio\"][\"array\"]\n",
    "sampling_rate = example[\"audio\"][\"sampling_rate\"]\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.waveshow(array, sr=sampling_rate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Audio\n",
    "\n",
    "minds = minds.cast_column(\"audio\", Audio(sampling_rate=16_000))\n",
    "minds[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_DURATION_IN_SECONDS = 20.0\n",
    "\n",
    "\n",
    "def is_audio_length_in_range(input_length):\n",
    "    return input_length < MAX_DURATION_IN_SECONDS\n",
    "\n",
    "\n",
    "# use librosa to get example's duration from the audio file\n",
    "new_column = [librosa.get_duration(filename=x) for x in minds[\"path\"]]\n",
    "minds = minds.add_column(\"duration\", new_column)\n",
    "\n",
    "# use ðŸ¤— Datasets' `filter` method to apply the filtering function\n",
    "minds = minds.filter(is_audio_length_in_range, input_columns=[\"duration\"])\n",
    "\n",
    "# remove the temporary helper column\n",
    "minds = minds.remove_columns([\"duration\"])\n",
    "minds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import WhisperFeatureExtractor\n",
    "\n",
    "feature_extractor = WhisperFeatureExtractor.from_pretrained(\n",
    "    \"openai/whisper-small\"\n",
    ")\n",
    "\n",
    "\n",
    "def prepare_dataset(example):\n",
    "    audio = example[\"audio\"]\n",
    "    features = feature_extractor(\n",
    "        audio[\"array\"], sampling_rate=audio[\"sampling_rate\"], padding=True\n",
    "    )\n",
    "    return features\n",
    "\n",
    "\n",
    "minds = minds.map(prepare_dataset)\n",
    "minds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "example = minds[0]\n",
    "input_features = example[\"input_features\"]\n",
    "\n",
    "plt.figure().set_figwidth(12)\n",
    "librosa.display.specshow(\n",
    "    np.asarray(input_features[0]),\n",
    "    x_axis=\"time\",\n",
    "    y_axis=\"mel\",\n",
    "    sr=feature_extractor.sampling_rate,\n",
    "    hop_length=feature_extractor.hop_length,\n",
    ")\n",
    "plt.colorbar()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoProcessor\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(\"openai/whisper-small\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gigaspeech = load_dataset(\"speechcolab/gigaspeech\", \"xs\", streaming=True)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
