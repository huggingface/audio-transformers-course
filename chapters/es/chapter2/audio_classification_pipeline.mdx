# Clasificaci贸n de audio con una pipeline

La clasificaci贸n de audio consiste en asignar o m谩s etiquetas a una grabaci贸n de audio basado en su contenido. Las etiquetas
pueden corresponder a diferentes categorias del sonido, como m煤sica, voz o ruido, o etiquetas m谩s especificas como canto de ave o
sonido de motor de carro.

Antes de entrar en los detalles sobre el funcionamiento de los transformers m谩s populares para audio, y antes de hacer fine-tunning 
de un modelo personalizado. Revisemos como se puede usar un modelo pre-entrenado para clasificaci贸n de audio con solo una lineas de 
c贸digo de   Transformers.

Usemos el mismo conjunto de [MINDS-14](https://huggingface.co/datasets/PolyAI/minds14) que hemos estado explorando en la 
unidad anterior. Como recordar谩s, MINDS-14 contiene grabaciones de personas haciendo preguntas a un sistema de banca electr贸nica 
en diferentes idiomas y dialectos, y tiene la etiqueta de `intent_class` para cada grabaci贸n. Podemos clasificar las grabaciones por 
la intenci贸n de la llamada.

Tal como hemos hecho antes, carguemos el subset de `en-AU` probar esta pipeline, y hagamos un upsampling a 16kHz que es 
la frecuencia de muestreo que el modelo espera. 

```py
from datasets import load_dataset
from datasets import Audio

minds = load_dataset("PolyAI/minds14", name="en-AU", split="train")
minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
```

Para clasificar una grabaci贸n de audio en alguna de las clases, podemos usar la pipeline de `audio-classification` que tiene  Transformers.
En nuestro caso, necesitamos un modelo que ha sido entrenado para clasificar la intenci贸n en donde especificamente hayan la base de datos de 
MINDS-14. Afortunadamente para nosotros, el Hub tiene un modelo que hace justamente esto. Carguemoslo usando la funci贸n `pipeline()`:

```py
from transformers import pipeline

classifier = pipeline(
    "audio-classification",
    model="anton-l/xtreme_s_xlsr_300m_minds14",
)
```
Esta pipeline espera los datos de audio como un array de Numpy. Todo el preprocesamiento de los datos de audio ser谩 hecha por 
la pipeline. Seleccionemos un ejemplo para probarlo:

```py
example = minds[0]
```
Si recuerdas la estructura del conjunto de datos, los datos en bruto de audio se almacenan en un array de Numpy en 
la columna `["audio"]["array"]`, que podemos pasar directamente al  `classifier`:

```py
classifier(example["audio"]["array"])
```

**Output:**
```out
[
    {"score": 0.9631525278091431, "label": "pay_bill"},
    {"score": 0.02819698303937912, "label": "freeze"},
    {"score": 0.0032787492964416742, "label": "card_issues"},
    {"score": 0.0019414445850998163, "label": "abroad"},
    {"score": 0.0008378693601116538, "label": "high_value_payment"},
]
```
Este modelo esta bastante seguro que la intenci贸n al llamar fue preguntar sobre el pago de una cuenta. Revisemos que la
etiqueta original para este ejemplo es:

```py
id2label = minds.features["intent_class"].int2str
id2label(example["intent_class"])
```

**Output:**
```out
"pay_bill"
```
隆Hurra! la etiqueta predecida por el modelo 隆era la correcta! Aqui tuvimos suerte de encontrar un modelo que tenia exactamente 
las etiquetas que necesitabamos. La mayoria de las veces, cuando estamos tratando con una tarea de clasificaci贸n, el conjunto de clases
de un modelo pre-entrenado no son exactamente las que necesitamos. En este caso, puedes hacer un fine-tunning del modelo pre entrenado
para "calibralo" a tu conjunto de clases. Aprenderemos como se puede hacer este proceso en las pr贸ximas unidad. Por ahora, echemos un
vistazo a otra tarea muy com煤n en el procesamiento del habla, _automatic speech recognition_.

