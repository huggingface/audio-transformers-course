# Introducci√≥n a los datos de audio

Por naturaleza, una onda sonora es una se√±al continua, es decir, que contiene un n√∫mero infinito de valores de la se√±al en un tiempo determinado.
Este es un problema para los dispositivos digitales que trabajan con un n√∫mero finito de valores. La onda sonora necesita ser convertida en 
un serie de valores discretos para que pueda ser procesada, almacenada y transmitida por un por un dispositivo digital, esta representaci√≥n discreta
se conoce como representaci√≥n digital.

Si exploras cualquier base de datos de audio, encontrar√°s archivos digitales con fragmentos de sonido que pueden contener narraciones o m√∫sica.
Puedes encontrar diferentes formatos de archivo como `.wav` (Waveform Audio File), `.flac` (Free Lossless Audio Codec) 
 y `.mp3` (MPEG-1 Audio Layer 3). Estos formatos difieren principalmente en como comprimen la representaci√≥n digital de la se√±al de audio. 

Examinemos como pasamos de una se√±al continua a una representaci√≥n digital. La se√±al ac√∫stica(an√°loga) es primero capturada por un micr√≥fono, 
que convierte las ondas sonoras en una se√±al electrica(Tambien an√°loga). La se√±al electrica es digitalizada por un 
conversor An√°logo-Digital(ADC) para tener una representaci√≥n digital a trav√©s del muestreo. 

## Muestreo y frecuencia de muestreo

El muestreo es el proceso de medir el valor de una se√±al continua en intervalos de tiempo fijos. La se√±al sampleada es discreta, 
ya que contiene un n√∫mero finito de valores de la se√±al.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/Signal_Sampling.png" alt="Signal sampling illustration">
</div>

*Ilustraci√≥n de un articulo de Wikipedia: [Muestreo(procesamiento de se√±al)](https://es.wikipedia.org/wiki/Muestreo_(se%C3%B1al))*

La **Tasa de Muestreo** (Tambien llamada frecuencia de muestreo) es el n√∫mero de muestras capturadas en un segundo y es medida en unidades 
de Hertz (Hz). Para tener un punto de referencia, un audio con calidad de CD tiene una frecuencia de muestreo de 44,100 Hz, es decir que se 
capturan 44,100 muestras por segundo. Existen archivos de audio Hi-Fi que utilizan frecuencias de muestreo de 192,000 hz o 192kHz. Una
frecuencia de muestreo com√∫nmente usada para entrenar modelos de voz es 16,0000 Hz o 16 kHz.

La elecci√≥n de la frecuencia de muestreo determina la frecuencia m√°s alta que puede ser representada digitalmente. Esto se conoce
como el limite de Nyquist y es exactamente la mitad de la se√±al de la frecuencia de muestreo. Las frecuencias audibles en la voz humana
estan por debajo de 8kHz, por lo que una frecuencia de 16kHz es suficiente. Usar una frecuencia de muestreo m√°s alta no va a capturar
m√°s informaci√≥n pero si a√±ade un costo computacional al procesamiento de estos archivos. En el caso contrario, elegir una frecuencia de 
muestreo muy baja puede resultar en perdidas de informaci√≥n. Audios de voz sampleados a 8kHz sonar√°n opacos, ya que las frecuencias por encima 
de 4kHz no pueden ser capturadas por esta frecuencia de muestreo. 

Es importante asegurar que todos los ejemplos de audio de la base de datos tienen la misma frecuencia de muestreo. Si planeas usar
tus propios audios para hacer fine-tunning de un modelo pre-entrenado, la frecuencia de muestreo de estos audios debe concordar con 
la frecuencia de muestreo con la que el modelo fue previamente entrenado. La frecuencia de muestreo determina el intervalo de tiempo
entre muestras de audio sucesivas, lo que tiene un efecto en la resoluci√≥n temporal de los datos de audio. Por ejemplo: un audio de 5 
segundos con una frecuencia de muestreo de 16,000 Hz ser√° representado como una serie de 80,000 valores, mientras que el mismo audio con 
una frecuencia de muestreo de 8,000hz ser√° representado como una serie de 40,000 valores. Los modelos de transformers para audio tratan los
ejemplos como secuencias y se basan en mecanismos de atenci√≥n para aprender del audio o de representaci√≥nes multimodales. Ya que la longitud 
de las secuencias difiere al usar frecuencias de muestreo diferentes, ser√° dificil para el modelo generalizar para diferentes frecuencias de muestro. 
**Resamplear** es el proceso de convertir una se√±al a otra frecuencia de muestreo, es parte de la secci√≥n de [preprosesamiento](preprocessing#resampling-the-audio-data) de datos de audio.


## Amplitud y profundidad de bits

Mientras que la frecuencia de muestreo te indica con qu√© frecuencia se toman las muestras, ¬øQu√© representan exactamente los valores en cada muestra?

El sonido se produce por cambios en la presi√≥n del aire a frecuencias audibles para los humanos. La **amplitud** de un sonido describe el nivel
de presi√≥n sonora en un momento dado y se mide en decibelios (dB). Percibimos la amplitud como volumen o intensidad del sonido. Por ejemplo, 
una voz normal al hablar est√° por debajo de los 60 dB, mientras que un concierto de rock puede llegar a los 125 dB, alcanzando 
los l√≠mites de la audici√≥n humana.

En el audio digital, cada muestra de audio registra la amplitud de la onda de audio en un momento espec√≠fico. La profundidad de bits de 
la muestra determina con qu√© precisi√≥n se puede describir este valor de amplitud. Cuanto mayor sea la profundidad de bits, m√°s fiel ser√° la
representaci√≥n digital a la onda de sonido continua original.

Las profundidades de bits de audio m√°s comunes son 16 bits y 24 bits. Cada una es una medida binaria que representa el n√∫mero de pasos posibles 
en los que se puede cuantificar el valor de amplitud al convertirlo de continuo a discreto: 65.536 pasos para el audio de 16 bits 
y 16.777.216 pasos para el audio de 24 bits. Debido a que la cuantificaci√≥n implica redondear el valor continuo a un valor discreto, el proceso 
de muestreo introduce ruido. Cuanto mayor sea la profundidad de bits, menor ser√° este ruido de cuantificaci√≥n. En la pr√°ctica, el ruido de 
cuantificaci√≥n del audio de 16 bits ya es lo suficientemente peque√±o como para ser audible, por lo que generalmente no es
 necesario utilizar profundidades de bits m√°s altas.

Tambi√©n es posible encontrarse con audio de 32 bits. Este almacena las muestras como valores de punto flotante, mientras que el audio
de 16 bits y 24 bits utiliza muestras enteras. La precisi√≥n de un valor de punto flotante de 32 bits es de 24 bits, lo que le otorga 
la misma profundidad de bits que el audio de 24 bits. Se espera que las muestras de audio de punto flotante se encuentren dentro del 
rango [-1.0, 1.0]. Dado que los modelos de aprendizaje autom√°tico trabajan naturalmente con datos de punto flotante, el audio debe convertirse
primero al formato de punto flotante antes de poder ser utilizado para entrenar el modelo. Veremos c√≥mo hacer esto en la pr√≥xima secci√≥n
 sobre Preprocesamiento.

Al igual que con las se√±ales de audio continuas, la amplitud del audio digital se expresa t√≠picamente en decibelios (dB). Dado que la audici√≥n
humana es de naturaleza logar√≠tmica, es decir, nuestros o√≠dos son m√°s sensibles a las peque√±as fluctuaciones en sonidos silenciosos que en
sonidos fuertes, el volumen de un sonido es m√°s f√°cil de interpretar si las amplitudes est√°n en decibelios, que tambi√©n son logar√≠tmicos.
La escala de decibelios para el audio real comienza en 0 dB, que representa el sonido m√°s silencioso posible que los humanos pueden escuchar,
y los sonidos m√°s fuertes tienen valores m√°s grandes. Sin embargo, para las se√±ales de audio digital, 0 dB es la amplitud m√°s alta posible, 
mientras que todas las dem√°s amplitudes son negativas. Como regla general: cada -6 dB implica una reducci√≥n a la mitad de la amplitud, y cualquier 
valor por debajo de -60 dB generalmente es inaudible a menos que subas mucho el volumen.

## Audio como forma de onda

Es posible que hayas visto los sonidos visualizados como una **forma de onda**(waveform), que representa los valores de las muestras a lo 
largo del tiempo y muestra los cambios en la amplitud del sonido. Esta representaci√≥n tambi√©n se conoce como la representaci√≥n en 
el dominio del tiempo del sonido.

Este tipo de visualizaci√≥n es √∫til para identificar caracter√≠sticas espec√≠ficas de la se√±al de audio, como la sincronizaci√≥n
de eventos de sonido individuales, la intensidad general de la se√±al y cualquier irregularidad o ruido presente en el audio.

Para graficar la forma de onda de una se√±al de audio, usamos una libreria de Python llamada `librosa`:

```bash
pip install librosa
```
Carguemos un ejemplo de la libreria llamado "trumpet":

```py
import librosa

array, sampling_rate = librosa.load(librosa.ex("trumpet"))
```

El ejemplo es cargado como una tupla formada por una serie temporal de valores de audio(llamado `array`) y la frecuencia de muestreo (`sampling_rate`).
Grafiquemos este sonido usando la funci√≥n `waveshow()` de librosa:

```py
import matplotlib.pyplot as plt
import librosa.display

plt.figure().set_figwidth(12)
librosa.display.waveshow(array, sr=sampling_rate)
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/waveform_plot.png" alt="Waveform plot">
</div>

Esta representaci√≥n grafica la amplitud de la se√±al en el eje y y el tiempo en el eje x. En otras palabras, cada punto corresponde 
a un √∫nico valor de muestra que se tom√≥ cuando se muestre√≥ este sonido. Tambi√©n es importante tener en cuenta que librosa devuelve
el audio en forma de valores de punto flotante y que los valores de amplitud se encuentran dentro del rango [-1.0, 1.0].

Visualizar el audio junto con escucharlo puede ser una herramienta √∫til para comprender los datos con los que est√°s trabajando. 
Puedes observar la forma de la se√±al, identificar patrones y aprender a detectar ruido o distorsi√≥n. 
Si preprocesas los datos de alguna manera, como normalizaci√≥n, remuestreo o filtrado, puedes confirmar visualmente que los pasos 
de preprocesamiento se hayan aplicado correctamente.

Despu√©s de entrenar un modelo, tambi√©n puedes visualizar las muestras donde se producen errores (por ejemplo, en una tarea de 
clasificaci√≥n de audio) para solucionar el problema. Esto te permitir√° depurar y entender mejor las √°reas en las que el modelo
puede tener dificultades o errores.

## El espectro de frecuencia

Otra forma de visualizar los datos de audio es graficar el espectro de frecuencia de una se√±al de audio,
 tambi√©n conocido como la representaci√≥n en el dominio de la frecuencia. El espectro se calcula utilizando la transformada 
 discreta de Fourier o DFT. Describe las frecuencias individuales que componen la se√±al y su intensidad.

Grafiquemos el espectro de frecuencia para el mismo sonido de trompeta mediante el c√°lculo de la transformada discreta de Fourier (DFT)
utilizando la funci√≥n rfft() de numpy. Si bien es posible trazar el espectro de toda la se√±al de audio, es m√°s √∫til observar una peque√±a
regi√≥n en su lugar. Aqu√≠ tomaremos la DFT de los primeros 4096 valores de muestra, que es aproximadamente la duraci√≥n de la primera nota
que se est√° tocando:

```py
import numpy as np

dft_input = array[:4096]

# calcular la DFT
window = np.hanning(len(dft_input))
windowed_input = dft_input * window
dft = np.fft.rfft(windowed_input)

# obtener la amplitud del espectro en decibeles
amplitude = np.abs(dft)
amplitude_db = librosa.amplitude_to_db(amplitude, ref=np.max)

# Obtener los bins de frecuencia
frequency = librosa.fft_frequencies(sr=sampling_rate, n_fft=len(dft_input))

plt.figure().set_figwidth(12)
plt.plot(frequency, amplitude_db)
plt.xlabel("Frequency (Hz)")
plt.ylabel("Amplitude (dB)")
plt.xscale("log")
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/spectrum_plot.png" alt="Spectrum plot">
</div>

Esto representa la intensidad de los diferentes componentes de frecuencia que est√°n presentes en este segmento de audio.
Los valores de frecuencia se encuentran en el eje x, generalmente representados en una escala logar√≠tmica, mientras que
las amplitudes se encuentran en el eje y.

El espectro de frecuencia que hemos graficado muestra varios picos. Estos picos corresponden a los arm√≥nicos de la nota que se est√° tocando, 
siendo los arm√≥nicos m√°s altos(en frecuencia) los m√°s silenciosos. Dado que el primer pico se encuentra alrededor de 620 Hz, este es el espectro
de frecuencia de una nota Mi‚ô≠.

La salida de la DFT es una matriz de n√∫meros complejos, compuestos por componentes reales e imaginarios. Tomar la magnitud con np.abs(dft)
extrae la informaci√≥n de amplitud del espectrograma. El √°ngulo entre los componentes reales e imaginarios proporciona el llamado espectro
de fase, pero esto a menudo se descarta en aplicaciones de aprendizaje autom√°tico.

Utilizamos `librosa.amplitude_to_db()` para convertir los valores de amplitud a la escala de decibelios, lo que facilita ver los detalles m√°s
sutiles en el espectro. A veces, las personas utilizan el **espectro de potencia**, que mide la energ√≠a en lugar de la amplitud; esto es simplemente
un espectro con los valores de amplitud elevados al cuadrado.+

<Tip>
üí° En la pr√°ctica, las personas utilizan indistintamente los t√©rminos FFT (Transformada R√°pida de Fourier) y DFT (Transformada Discreta de Fourier), 
ya que la FFT es la √∫nica forma eficiente de calcular la DFT en una computadora.
</Tip>

El espectro de frecuencia de una se√±al de audio contiene exactamente la misma informaci√≥n que su representaci√≥n en el dominio
del tiempo(forma de onda); simplemente son dos formas diferentes de ver los mismos datos (en este caso, los primeros 4096 valores
de muestra del sonido de trompeta). Mientras que la forma de onda representa la amplitud de la se√±al de audio a lo largo del tiempo,
el espectro visualiza las amplitudes de las frecuencias individuales en un punto fijo en el tiempo.

## Espectrograma

¬øY si queremos ver c√≥mo cambian las frecuencias en una se√±al de audio? La trompeta toca varias notas que tienen diferentes frecuencias.
 El problema es que el espectro solo es una foto fija de las frecuencias en un instante determinado. La soluci√≥n es tomar m√∫ltiples DFT, 
 cada una abarcando un peque√±o fragmento de la se√±al, y luego apilar los espectros resultantes en un **espectrograma**.

Un espectrograma grafica el contenido frecuencial de una se√±al de audio a medida que cambia en el tiempo. Esto nos permite ver en la 
misma gr√°fica la informaci√≥n de tiempo, frecuencia y amplitud. El algoritmo que permite hacer este representaci√≥n se conoce como STFT o 
Transformada de tiempo corto de Fourier.

El espectrograma es una de las herramientas m√°s √∫tiles disponibles. Por ejemplo, cuando estamos trabajando con una grabaci√≥n de m√∫sica, 
podemos ver como contribuye cada instrumento y las voces al sonido general. En el habla, se pueden identificar los difrerentes
sonidos de las vocales ya que cada vocal esta caracterizada por frecuencias particulares. 

Grafiquemos ahora un espectrograma del mismo sonido de trompeta, usando las funciones de librosa `stft()` y `specshow()`:

```py
import numpy as np

D = librosa.stft(array)
S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)

plt.figure().set_figwidth(12)
librosa.display.specshow(S_db, x_axis="time", y_axis="hz")
plt.colorbar()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/spectrogram_plot.png" alt="Spectrogram plot">
</div>

En este gr√°fico, el eje x representa el tiempo al igual que en la visulizaci√≥n de la forma de onda sin embargo el eje y ahora representa la 
frecuencia en hertz(Hz). La intensidad del color representa el nivel en decibelios (dB) de las componentes de frecuencia en cada punto del tiempo.

El espectrograma es creado al tomar peque√±os segmentos de la se√±al de audio, comunmente de unos cuantos milisegundos y calculando
la transformada discreta de Fourier de cada segmento para obtener el espectro en frecuencia. Estos espectros se concatenan a lo 
largo del eje temporal para crear un espectrograma. Cada columna en la imagen corresponde a un espectro de frecuencia, Por defecto, 
la funci√≥n `librosa.stft()` divide la se√±al en segmentos de 2048 muestras, lo que ofrece un buen resultado
para la resoluci√≥n en frecuencia y la resoluci√≥n temporal.

Dado que el espectrograma y la forma de onda son diferentes representaciones de los mismos datos, es posible convertir el 
espectrograma nuevamente en la forma de onda original utilizando la STFT inversa (transformada de Fourier de tiempo corto inversa). 
Sin embargo, esto requiere tanto la informaci√≥n de amplitud como la informaci√≥n de fase. Si el espectrograma fue generado por un
modelo de aprendizaje autom√°tico, t√≠picamente solo se genera la informaci√≥n de amplitud. En ese caso, podemos utilizar un algoritmo
de reconstrucci√≥n de fase cl√°sico como el algoritmo de Griffin-Lim, o utilizar una red neuronal llamada vocoder, para reconstruir una 
forma de onda a partir del espectrograma.

Los espectrogramas no solo se utilizan para visualizaci√≥n. Muchos modelos de aprendizaje autom√°tico toman espectrogramas como entrada, 
en lugar de formas de onda, y producen espectrogramas como salida.

Ahora que sabemos qu√© es un espectrograma y c√≥mo se genera, echemos un vistazo a una variante ampliamente utilizada en el procesamiento 
del habla: el espectrograma de mel.

## Espectrograma de Mel

Un espectrograma mel es una variante del espectrograma que se utiliza com√∫nmente en el procesamiento del habla y en tareas de aprendizaje
 autom√°tico. Es similar a un espectrograma en el sentido de que muestra el contenido de frecuencia de una se√±al de audio a lo largo del tiempo,
  pero en un eje de frecuencia diferente.

En un espectrograma est√°ndar, el eje de frecuencia es lineal y se mide en hercios (Hz). Sin embargo, el sistema auditivo humano
es m√°s sensible a los cambios en las frecuencias bajas que en las frecuencias altas, y esta sensibilidad disminuye de manera logar√≠tmica
a medida que la frecuencia aumenta. La escala mel es una escala perceptual que aproxima la respuesta de frecuencia no lineal del o√≠do humano.

Para crear un espectrograma de mel, se utiliza la STFT de la misma manera que vimos antes, dividiendo el audio en segmentos cortos para 
obtener una secuencia de espectros de frecuencia. Adem√°s, cada espectro se pasa a trav√©s de un conjunto de filtros(banco de filtros de mel) 
para transformar las frecuencias a la escala de mel.

Obsrevemos como podemos obtener el espectrograma de mel usando la funci√≥n `melspectrogram()`  de librosa, que realiza todos los pasos anteriores:

```py
S = librosa.feature.melspectrogram(y=array, sr=sampling_rate, n_mels=128, fmax=8000)
S_dB = librosa.power_to_db(S, ref=np.max)

plt.figure().set_figwidth(12)
librosa.display.specshow(S_dB, x_axis="time", y_axis="mel", sr=sampling_rate, fmax=8000)
plt.colorbar()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/mel-spectrogram.png" alt="Mel spectrogram plot">
</div>

En el ejemplo anterior, `n_mels` representa el n√∫mero de bandas de mel que se generar√°n. Las bandas de mel definen un conjunto de
rangos de frecuencia que dividen el espectro en componentes significativos desde el punto de vista perceptual, utilizando un
conjunto de filtros cuya forma y espaciado se eligen para imitar la forma en que el o√≠do humano responde a diferentes frecuencias.
Los valores comunes para `n_mels` son 40 o 80. `fmax` indica la frecuencia m√°s alta (en Hz) que nos interesa.

Al igual que con un espectrograma regular, es pr√°ctica com√∫n expresar la intensidad de los componentes de frecuencia de mel en decibelios.
 Esto se conoce com√∫nmente como un **espectrograma logar√≠tmico de mel**, porque la conversi√≥n a decibelios implica una operaci√≥n logar√≠tmica. 
 El ejemplo anterior se us√≥ `librosa.power_to_db()` ya que la funci√≥n `librosa.feature.melspectrogram()` crea un espectrograma de potencia.


<Tip>

üí° ¬°No todos los espectrogramas mel son iguales! Existen dos variantes comumente usadas de las escalas de mel("htk" y "slaney"),
, y en lugar del espectrograma de potencia, se puede estar usando el espectrograma de amplitud. El c√°lculo de un espectrograma 
logar√≠tmico de mel no siempre usa decibelios reales, puede que se haya aplicado solamente la funci√≥n `log`. Por lo tanto, 
si un modelo de aprendizaje autom√°tico espera un espectrograma de mel como entrada, verifica que est√©s calcul√°ndolo de la misma manera
 para asegurarte de que sea compatible.
</Tip>

La creaci√≥n de un espectrograma mel es una operaci√≥n con p√©rdidas, ya que implica filtrar la se√±al. Convertir un espectrograma de mel de
nuevo en una forma de onda es m√°s dif√≠cil que hacerlo para un espectrograma regular, ya que requiere estimar las frecuencias que se eliminaron.
Es por eso que se necesitan modelos de aprendizaje autom√°tico como el vocoder HiFiGAN para producir una forma de onda a partir de un espectrograma de mel.

En comparaci√≥n con un espectrograma est√°ndar, un espectrograma mel captura caracter√≠sticas m√°s significativas de la se√±al de audio para la percepci√≥n humana,
 lo que lo convierte en una opci√≥n popular en tareas como el reconocimiento de voz, la identificaci√≥n de hablantes y la clasificaci√≥n de g√©neros musicales.

Ahora que sabes c√≥mo visualizar datos de audio, trata de ver c√≥mo se ven tus sonidos favoritos. :)