# –ü—Ä–µ–ø—Ä–æ—Ü–µ—Å—Å–∏–Ω–≥ –Ω–∞–±–æ—Ä–∞ –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã—Ö

–ó–∞–≥—Ä—É–∑–∫–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö —Å –ø–æ–º–æ—â—å—é ü§ó Datasets - —ç—Ç–æ —Ç–æ–ª—å–∫–æ –ø–æ–ª–æ–≤–∏–Ω–∞ —É–¥–æ–≤–æ–ª—å—Å—Ç–≤–∏—è. –ï—Å–ª–∏ –≤—ã –ø–ª–∞–Ω–∏—Ä—É–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–µ –ª–∏–±–æ –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏, –ª–∏–±–æ –¥–ª—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è
–∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å –¥–∞–Ω–Ω—ã–µ. –í –æ–±—â–µ–º —Å–ª—É—á–∞–µ —ç—Ç–æ –≤–∫–ª—é—á–∞–µ—Ç –≤ —Å–µ–±—è —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:

* –ü–µ—Ä–µ–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã—Ö
* –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö
* –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã—Ö –≤ –æ–∂–∏–¥–∞–µ–º—ã–π –º–æ–¥–µ–ª—å—é  —Ñ–æ—Ä–º–∞—Ç –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö

## –ü–µ—Ä–µ–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏—è –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã—Ö

–§—É–Ω–∫—Ü–∏—è `load_dataset` –∑–∞–≥—Ä—É–∂–∞–µ—Ç –∞—É–¥–∏–æ–ø—Ä–∏–º–µ—Ä—ã —Å —Ç–æ–π —á–∞—Å—Ç–æ—Ç–æ–π –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏, —Å –∫–æ—Ç–æ—Ä–æ–π –æ–Ω–∏ –±—ã–ª–∏ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω—ã. –≠—Ç–æ –Ω–µ
–≤—Å–µ–≥–¥–∞ —Ç–∞ —á–∞—Å—Ç–æ—Ç–∞ –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏, –∫–æ—Ç–æ—Ä–∞—è –æ–∂–∏–¥–∞–µ—Ç—Å—è –º–æ–¥–µ–ª—å—é, –∫–æ—Ç–æ—Ä—É—é –≤—ã –ø–ª–∞–Ω–∏—Ä—É–µ—Ç–µ –æ–±—É—á–∞—Ç—å –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –¥–ª—è –∏–Ω—Ñ–µ—Ä–µ–Ω—Å–∞. –ï—Å–ª–∏ –µ—Å—Ç—å —Ä–∞—Å—Ö–æ–∂–¥–µ–Ω–∏–µ –º–µ–∂–¥—É
—á–∞—Å—Ç–æ—Ç–æ–π –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏, –º–æ–∂–Ω–æ –ø–µ—Ä–µ–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∑–≤—É–∫ –¥–æ –æ–∂–∏–¥–∞–µ–º–æ–π –º–æ–¥–µ–ª—å—é —á–∞—Å—Ç–æ—Ç—ã –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏.

–ë–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ –∏–º–µ—é—â–∏—Ö—Å—è –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –±—ã–ª–∏ –æ–±—É—á–µ–Ω—ã –Ω–∞ –∞—É–¥–∏–æ–¥–∞–Ω–Ω—ã—Ö —Å —á–∞—Å—Ç–æ—Ç–æ–π –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ 16 –∫–ì—Ü.
–ö–æ–≥–¥–∞ –º—ã –∏—Å—Å–ª–µ–¥–æ–≤–∞–ª–∏ –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö MINDS-14, –≤—ã –º–æ–≥–ª–∏ –∑–∞–º–µ—Ç–∏—Ç—å, —á—Ç–æ –æ–Ω —Å—ç–º–ø–ª–∏—Ä–æ–≤–∞–Ω —Å —á–∞—Å—Ç–æ—Ç–æ–π 8 –∫–ì—Ü, —á—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –Ω–∞–º, —Å–∫–æ—Ä–µ–µ –≤—Å–µ–≥–æ, –ø–æ—Ç—Ä–µ–±—É–µ—Ç—Å—è
—É–≤–µ–ª–∏—á–∏—Ç—å —á–∞—Å—Ç–æ—Ç—É –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏.

–ß—Ç–æ–±—ã —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ, –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –º–µ—Ç–æ–¥ ü§ó Datasets `cast_column`. –≠—Ç–∞ –æ–ø–µ—Ä–∞—Ü–∏—è –Ω–µ –∏–∑–º–µ–Ω—è–µ—Ç –∑–≤—É–∫ –Ω–µ–ø–æ—Å—Ä–µ–¥—Å—Ç–≤–µ–Ω–Ω–æ –≤ –Ω–∞–±–æ—Ä–µ –¥–∞–Ω–Ω—ã—Ö (in-place), –∞ –¥–∞–µ—Ç —Å–∏–≥–Ω–∞–ª
datasets –¥–ª—è –ø–µ—Ä–µ–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏ –∞—É–¥–∏–æ–ø—Ä–∏–º–µ—Ä–æ–≤ "–Ω–∞ –ª–µ—Ç—É" –ø—Ä–∏ –∏—Ö –∑–∞–≥—Ä—É–∑–∫–µ. –°–ª–µ–¥—É—é—â–∏–π –∫–æ–¥ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç —á–∞—Å—Ç–æ—Ç—É –¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∞—Ü–∏–∏
—Ä–∞–≤–Ω–æ–π 16 –∫–ì—Ü:

```py
from datasets import Audio

minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
```

–ü–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏–º –ø–µ—Ä–≤—ã–π –∞—É–¥–∏–æ–ø—Ä–∏–º–µ—Ä –∏–∑ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö MINDS-14 –∏ –ø—Ä–æ–≤–µ—Ä–∏–º, —á—Ç–æ –æ–Ω –±—ã–ª –ø–µ—Ä–µ–¥–∏—Å–∫—Ä–µ—Ç–∏–∑–∏—Ä–æ–≤–∞–Ω –¥–æ –Ω—É–∂–Ω–æ–π `sampling rate`:

```py
minds[0]
```

**Output:**
```out
{
    "path": "/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-AU~PAY_BILL/response_4.wav",
    "audio": {
        "path": "/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-AU~PAY_BILL/response_4.wav",
        "array": array(
            [
                2.0634243e-05,
                1.9437837e-04,
                2.2419340e-04,
                ...,
                9.3852862e-04,
                1.1302452e-03,
                7.1531429e-04,
            ],
            dtype=float32,
        ),
        "sampling_rate": 16000,
    },
    "transcription": "I would like to pay my electricity bill using my card can you please assist",
    "intent_class": 13,
}
```

–í—ã –º–æ–∂–µ—Ç–µ –∑–∞–º–µ—Ç–∏—Ç—å, —á—Ç–æ –∑–Ω–∞—á–µ–Ω–∏—è –º–∞—Å—Å–∏–≤–∞ —Ç–µ–ø–µ—Ä—å —Ç–∞–∫–∂–µ –æ—Ç–ª–∏—á–∞—é—Ç—Å—è. –≠—Ç–æ —Å–≤—è–∑–∞–Ω–æ —Å —Ç–µ–º, —á—Ç–æ —Ç–µ–ø–µ—Ä—å –¥–ª—è
–∫–∞–∂–¥–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è –∞–º–ø–ª–∏—Ç—É–¥—ã –º—ã –∏–º–µ–µ–º –≤ –¥–≤–∞ —Ä–∞–∑–∞ –±–æ–ª—å—à–µ –∑–Ω–∞—á–µ–Ω–∏–π —á–µ–º —Ä–∞–Ω—å—à–µ.

<Tip>
üí° Some background on resampling: If an audio signal has been sampled at 8 kHz, so that it has 8000 sample readings per
second, we know that the audio does not contain any frequencies over 4 kHz. This is guaranteed by the Nyquist sampling
theorem. Because of this, we can be certain that in between the sampling points the original continuous signal always
makes a smooth curve. Upsampling to a higher sampling rate is then a matter of calculating additional sample values that go in between
the existing ones, by approximating this curve. Downsampling, however, requires that we first filter out any frequencies
that would be higher than the new Nyquist limit, before estimating the new sample points. In other words, you can't
downsample by a factor 2x by simply throwing away every other sample ‚Äî this will create distortions in the signal called
aliases. Doing resampling correctly is tricky and best left to well-tested libraries such as librosa or ü§ó Datasets.
</Tip>

## Filtering the dataset

You may need to filter the data based on some criteria. One of the common cases involves limiting the audio examples to a
certain duration. For instance, we might want to filter out any examples longer than 20s to prevent out-of-memory errors
when training a model.

We can do this by using the  ü§ó Datasets' `filter` method and passing a function with filtering logic to it. Let's start by writing a
function that indicates which examples to keep and which to discard. This function, `is_audio_length_in_range`,
returns `True` if a sample is shorter than 20s, and `False` if it is longer than 20s.

```py
MAX_DURATION_IN_SECONDS = 20.0


def is_audio_length_in_range(input_length):
    return input_length < MAX_DURATION_IN_SECONDS
```

The filtering function can be applied to a dataset's column but we do not have a column with audio track duration in this
dataset. However, we can create one, filter based on the values in that column, and then remove it.

```py
# use librosa to get example's duration from the audio file
new_column = [librosa.get_duration(path=x) for x in minds["path"]]
minds = minds.add_column("duration", new_column)

# use ü§ó Datasets' `filter` method to apply the filtering function
minds = minds.filter(is_audio_length_in_range, input_columns=["duration"])

# remove the temporary helper column
minds = minds.remove_columns(["duration"])
minds
```

**Output:**
```out
Dataset({features: ["path", "audio", "transcription", "intent_class"], num_rows: 624})
```

We can verify that dataset has been filtered down from 654 examples to 624.

## Pre-processing audio data

One of the most challenging aspects of working with audio datasets is preparing the data in the right format for model
training. As you saw, the raw audio data comes as an array of sample values. However, pre-trained models, whether you use them
for inference, or want to fine-tune them for your task, expect the raw data to be converted into input features. The
requirements for the input features may vary from one model to another ‚Äî they depend on the model's architecture, and the data it was
pre-trained with. The good news is, for every supported audio model, ü§ó Transformers offer a feature extractor class
that can convert raw audio data into the input features the model expects.

So what does a feature extractor do with the raw audio data? Let's take a look at [Whisper](https://huggingface.co/papers/2212.04356)'s
feature extractor to understand some common feature extraction transformations. Whisper is a pre-trained model for
automatic speech recognition (ASR) published in September 2022 by Alec Radford et al. from OpenAI.

First, the Whisper feature extractor pads/truncates a batch of audio examples such that all
examples have an input length of 30s. Examples shorter than this are padded to 30s by appending zeros to the end of the
sequence (zeros in an audio signal correspond to no signal or silence). Examples longer than 30s are truncated to 30s.
Since all elements in the batch are padded/truncated to a maximum length in the input space, there is no need for an attention
mask. Whisper is unique in this regard, most other audio models require an attention mask that details
where sequences have been padded, and thus where they should be ignored in the self-attention mechanism. Whisper is
trained to operate without an attention mask and infer directly from the speech signals where to ignore the inputs.

The second operation that the Whisper feature extractor performs is converting the padded audio arrays to log-mel spectrograms.
As you recall, these spectrograms describe how the frequencies of a signal change over time, expressed on the mel scale
and measured in decibels (the log part) to make the frequencies and amplitudes more representative of human hearing.

All these transformations can be applied to your raw audio data with a couple of lines of code. Let's go ahead and load
the feature extractor from the pre-trained Whisper checkpoint to have ready for our audio data:

```py
from transformers import WhisperFeatureExtractor

feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-small")
```

Next, you can write a function to pre-process a single audio example by passing it through the `feature_extractor`.

```py
def prepare_dataset(example):
    audio = example["audio"]
    features = feature_extractor(
        audio["array"], sampling_rate=audio["sampling_rate"], padding=True
    )
    return features
```

We can apply the data preparation function to all of our training examples using ü§ó Datasets' map method:

```py
minds = minds.map(prepare_dataset)
minds
```

**Output:**
```out
Dataset(
    {
        features: ["path", "audio", "transcription", "intent_class", "input_features"],
        num_rows: 624,
    }
)
```

As easy as that, we now have log-mel spectrograms as `input_features` in the dataset.

Let's visualize it for one of the examples in the `minds` dataset:

```py
import numpy as np

example = minds[0]
input_features = example["input_features"]

plt.figure().set_figwidth(12)
librosa.display.specshow(
    np.asarray(input_features[0]),
    x_axis="time",
    y_axis="mel",
    sr=feature_extractor.sampling_rate,
    hop_length=feature_extractor.hop_length,
)
plt.colorbar()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/log_mel_whisper.png" alt="Log mel spectrogram plot">
</div>

Now you can see what the audio input to the Whisper model looks like after preprocessing.

The model's feature extractor class takes care of transforming raw audio data to the format that the model expects. However,
many tasks involving audio are multimodal, e.g. speech recognition. In such cases ü§ó Transformers also offer model-specific
tokenizers to process the text inputs. For a deep dive into tokenizers, please refer to our [NLP course](https://huggingface.co/course/chapter2/4).

You can load the feature extractor and tokenizer for Whisper and other multimodal models separately, or you can load both via
a so-called processor. To make things even simpler, use `AutoProcessor` to load a model's feature extractor and processor from a
checkpoint, like this:

```py
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("openai/whisper-small")
```

Here we have illustrated the fundamental data preparation steps. Of course, custom data may require more complex preprocessing.
In this case, you can extend the function `prepare_dataset` to perform any sort of custom data transformations. With ü§ó Datasets,
if you can write it as a Python function, you can [apply it](https://huggingface.co/docs/datasets/audio_process) to your dataset!
