# Introdu√ß√£o a dados de √°udio

Por natureza, uma onda sonora √© um sinal cont√≠nuo, o que significa que cont√©m um n√∫mero infinito de valores de sinal em um determinado tempo. Isso traz problemas para dispositivos digitais que esperam arrays finitos. Para ser processada, armazenada e transmitida por dispositivos digitais, a onda sonora cont√≠nua precisa ser convertida em uma s√©rie de valores discretos, conhecidos como representa√ß√£o digital.

Se voc√™ olhar para qualquer dataset de √°udio, encontrar√° arquivos digitais com trechos de som, como narra√ß√£o de texto ou m√∫sica. Voc√™ pode encontrar diferentes formatos de arquivo, como `.wav` (Waveform Audio File), `.flac` (Free Lossless Audio Codec) e `.mp3` (MPEG-1 Audio Layer 3). Esses formatos diferem principalmente na forma como comprimem a representa√ß√£o digital do sinal de √°udio.

Vamos dar uma olhada em como sa√≠mos de um sinal cont√≠nuo para essa representa√ß√£o. O sinal anal√≥gico √© primeiro capturado por um microfone, que converte as ondas sonoras em um sinal el√©trico. O sinal el√©trico √© ent√£o digitalizado por um Conversor Anal√≥gico-Digital para obter a representa√ß√£o digital por meio da amostragem.

## Amostragem e taxa de amostragem

Amostragem (sampling) √© o processo de medir o valor de um sinal cont√≠nuo em etapas (amostras) que possuem o mesmo tempo. A forma da onda que foi amostrada √© _discreta_, pois cont√©m um n√∫mero finito dos valores do sinal em intervalos uniformes.

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/Signal_Sampling.png" alt="Ilustra√ß√£o de amostragem de sinal">
</div>

*Ilustra√ß√£o deste artigo da Wikipedia: [Amostragem (processamento de sinal)](https://en.wikipedia.org/wiki/Sampling_(signal_processing))*

A **taxa de amostragem** (tamb√©m chamada de frequ√™ncia de amostragem, sampling rate ou sampling frequency) √© o n√∫mero de amostras feitas em um segundo e √© medida em hertz (Hz). Para voc√™ ter ideia, √°udio com qualidade de CD tem uma taxa de amostragem de 44.100 Hz, o que significa que as amostras s√£o feitas 44.100 vezes por segundo. Para compara√ß√£o, o √°udio de alta resolu√ß√£o tem uma taxa de amostragem de 192.000 Hz ou 192 kHz. Uma taxa de amostragem comum usada no treinamento de modelos de fala √© de 16.000 Hz ou 16 kHz.

A escolha da taxa de amostragem determina, principalmente, a maior frequ√™ncia que pode ser capturada do sinal. Isso tamb√©m √© conhecido como o limite de Nyquist e √© exatamente a metade da taxa de amostragem. As frequ√™ncias aud√≠veis na fala humana est√£o abaixo de 8 kHz e, portanto, amostrar a fala a 16 kHz √© suficiente. Usar uma taxa de amostragem mais alta n√£o capturar√° mais informa√ß√µes e apenas leva a um aumento no custo computacional de processar tais arquivos. Por outro lado, amostrar √°udio em uma taxa de amostragem muito baixa resultar√° em perda de informa√ß√£o. A fala amostrada a 8 kHz soar√° abafada, pois as frequ√™ncias mais altas n√£o podem ser capturadas nessa taxa.

√â importante garantir que todos os exemplos de √°udio no seu dataset tenham a mesma taxa de amostragem ao trabalhar em qualquer processamento de √°udio. Se voc√™ planeja usar dados de √°udio personalizados para ajustar um modelo pr√©-treinado, a taxa de amostragem dos seus dados deve corresponder √† taxa de amostragem dos dados em que o modelo foi pr√©-treinado. A taxa de amostragem determina o intervalo de tempo entre amostras de √°udio sucessivas, o que impacta a resolu√ß√£o temporal dos dados de √°udio. Considere um exemplo: um som de 5 segundos a uma taxa de amostragem de 16.000 Hz ser√° representado como uma s√©rie de 80.000 valores (16.000 x 5), enquanto o mesmo som de 5 segundos a uma taxa de amostragem de 8.000 Hz ser√° representado como uma s√©rie de 40.000 valores. Modelos Transformers que resolvem tarefas de √°udio tratam exemplos como sequ√™ncias e dependem de mecanismos de aten√ß√£o para aprender √°udio ou representa√ß√µes multimodais. Como as sequ√™ncias s√£o diferentes para exemplos de √°udio em diferentes taxas de amostragem, ser√° desafiador para os modelos generalizarem entre taxas de amostragem. **Reamostragem** (Resampling) √© o processo de fazer as taxas de amostragem se igualarem, e faz parte do [pr√©-processamento](preprocessing#resampling-the-audio-data) dos dados de √°udio.

## Amplitude e profundidade de bits (bit depth)

Enquanto a taxa de amostragem diz com que frequ√™ncia as amostras s√£o feitas, o que exatamente s√£o os valores em cada amostra?

O som √© feito por mudan√ßas na press√£o do ar em frequ√™ncias aud√≠veis para humanos. A **amplitude** de um som descreve o n√≠vel de press√£o sonora em um dado momento e √© medida em decib√©is (dB). Percebemos a amplitude como volume. Para lhe dar um exemplo, uma voz falando normalmente est√° abaixo de 60 dB, e um show de rock pode estar em torno de 125 dB, for√ßando os limites da audi√ß√£o humana.

No √°udio digital, cada amostra do √°udio √© o valor da amplitude da onda sonora em um ponto no tempo. A **profundidade de bits** (bit depth) da amostra determina o qu√£o preciso esse valor pode ser. Quanto maior a profundidade de bits, mais fiel ser√° a representa√ß√£o digital da onda sonora cont√≠nua original.

As profundidades de bit de √°udio mais comuns s√£o 16 bits e 24 bits. Cada uma representa o n√∫mero m√°ximo de valores distintos que podem ser registrados em cada amostra: 65.536 valores para √°udio de 16 bits, impressionantes 16.777.216 valores para √°udio de 24 bits. Devido ao fato que essa quantiza√ß√£o do valor cont√≠nuo para o discreto envolve arredondamento, o processo de amostragem causa ru√≠do. Quanto maior a profundidade de bits, menor esse ru√≠do de quantiza√ß√£o. Na pr√°tica, o ru√≠do de quantiza√ß√£o do √°udio de 16 bits j√° √© pequeno o suficiente para ser inaud√≠vel, e usar profundidades de bits maiores geralmente n√£o √© necess√°rio.

Voc√™ tamb√©m pode encontrar √°udio de 32 bits. Isso armazena as amostras como valores de ponto flutuante (decimal), enquanto o √°udio de 16 bits e 24 bits usa amostras inteiras. A precis√£o de um valor de ponto flutuante de 32 bits √© de 24 bits, dando-lhe a mesma profundidade de bits que o √°udio de 24 bits. Amostras de √°udio de ponto flutuante s√£o esperadas para estar dentro do intervalo [-1.0, 1.0]. Como os modelos de machine learning trabalham naturalmente com dados de ponto flutuante, o √°udio deve primeiro ser convertido para o formato de ponto flutuante antes de poder ser usado para treinar o modelo. Veremos como fazer isso na pr√≥xima se√ß√£o sobre [Pr√©-processamento](preprocessing).

Assim como com sinais de √°udio cont√≠nuos, a amplitude do √°udio digital √© tipicamente expressa em decib√©is (dB). Como a audi√ß√£o humana √© logar√≠tmica por natureza ‚Äî nossos ouvidos s√£o mais sens√≠veis a pequenas flutua√ß√µes em sons silenciosos do que em sons altos ‚Äî a intensidade de um som √© mais f√°cil de interpretar se as amplitudes estiverem em decib√©is, que tamb√©m s√£o logar√≠tmicos. A escala de decib√©is para √°udio do mundo real come√ßa em 0 dB, que representa o som mais baixo poss√≠vel que os humanos podem ouvir, e sons mais altos t√™m valores maiores. No entanto, para sinais de √°udio digitais, 0 dB √© a amplitude mais alta poss√≠vel, enquanto todas as outras amplitudes s√£o negativas. Como uma regra r√°pida: a cada -6 dB √© uma redu√ß√£o pela metade da amplitude, e qualquer coisa abaixo de -60 dB geralmente √© inaud√≠vel a menos que voc√™ realmente aumente o volume.

## √Åudio como uma forma de onda

Voc√™ j√° pode ter visto sons visualizados como uma **forma de onda**, que tra√ßa os valores das amostras ao longo do tempo e ilustra as mudan√ßas na amplitude do som. Isso tamb√©m √© conhecido como a representa√ß√£o do *dom√≠nio do tempo* do som.

Esse tipo de visualiza√ß√£o √© √∫til para identificar caracter√≠sticas espec√≠ficas do sinal de √°udio, como o momento de eventos sonoros individuais, o volume geral do sinal e quaisquer irregularidades ou ru√≠dos presentes no √°udio.

Para tra√ßar a forma de onda de um sinal de √°udio, podemos usar uma biblioteca Python chamada `librosa`:

```bash
pip install librosa
```

Vamos pegar um exemplo de som chamado "trompete" que vem com a biblioteca:

```py
import librosa

array, sampling_rate = librosa.load(librosa.ex("trumpet"))
```

O exemplo √© carregado como uma tupla contendo uma sequencia temporal de √°udio (aqui chamamos de `array`) e a taxa de amostragem (`sampling_rate`). Vamos dar uma olhada na forma de onda deste som usando a fun√ß√£o `waveshow()` da librosa:

```py
import matplotlib.pyplot as plt
import librosa.display

plt.figure().set_figwidth(12)
librosa.display.waveshow(array, sr=sampling_rate)
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/waveform_plot.png" alt="Forma de onda">
</div>

Isso coloca a amplitude do sinal no eixo y e o tempo ao longo do eixo x. Em outras palavras, cada ponto corresponde a um √∫nico valor de amostra que foi feito quando este som foi amostrado. Tamb√©m observe que a librosa j√° retorna o √°udio como valores de ponto flutuante e que os valores de amplitude est√£o de fato dentro do intervalo [-1,0, 1,0].

Visualizar o √°udio junto com ouvi-lo pode ser uma ferramenta √∫til para entender os dados com os quais voc√™ est√° trabalhando. Voc√™ pode ver a forma do sinal, observar padr√µes, aprender a identificar ru√≠dos ou distor√ß√£o. Se voc√™ pr√©-processar dados de alguma forma, como normaliza√ß√£o, reamostragem ou filtragem, voc√™ pode confirmar visualmente que as etapas de pr√©-processamento foram aplicadas conforme esperado. Ap√≥s treinar um modelo, voc√™ tamb√©m pode visualizar amostras onde ocorrem erros (por exemplo, em tarefa de classifica√ß√£o de √°udio) para fazer o debug do problema.

## O espectro de frequ√™ncia

Outra maneira de visualizar dados de √°udio √© plotar o **espectro de frequ√™ncia** de um sinal de √°udio, tamb√©m conhecido como a representa√ß√£o do *dom√≠nio da frequ√™ncia*. O espectro √© calculado usando a transformada de Fourier discreta ou DFT. Ele descreve as frequ√™ncias individuais que comp√µem o sinal e qu√£o fortes elas s√£o.

Vamos plotar o espectro de frequ√™ncia para o mesmo som de trompete, fazendo a DFT usando a fun√ß√£o `rfft()` do numpy. Embora seja poss√≠vel plotar o espectro de todo o som, √© mais √∫til olhar para uma pequena regi√£o em vez disso. Aqui vamos fazer a DFT sobre as primeiras 4096 amostras, que √© aproximadamente o comprimento da primeira nota sendo tocada:

```py
import numpy as np

dft_input = array[:4096]

# calcula a DFT
window = np.hanning(len(dft_input))
windowed_input = dft_input * window
dft = np.fft.rfft(windowed_input)

# obt√©m as amplitude do espectro, em decibeis
amplitude = np.abs(dft)
amplitude_db = librosa.amplitude_to_db(amplitude, ref=np.max)

# obt√©m a frquency bins (faixas de frequencia)
frequency = librosa.fft_frequencies(sr=sampling_rate, n_fft=len(dft_input))

plt.figure().set_figwidth(12)
plt.plot(frequency, amplitude_db)
plt.xlabel("Frequency (Hz)")
plt.ylabel("Amplitude (dB)")
plt.xscale("log")
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/spectrum_plot.png" alt="Spectrum plot">
</div>

Isso mostra um gr√°fico com a for√ßa de v√°rias frequ√™ncias que est√£o presentes neste trecho de √°udio. Os valores da frequ√™ncia est√£o no eixo x, geralmente plotados em uma escala logar√≠tmica, enquanto suas amplitudes est√£o no eixo y.

O espectro de frequ√™ncia que plotamos mostra v√°rios picos. Esses picos correspondem aos harm√¥nicos da nota que est√° sendo tocada, com os harm√¥nicos superiores sendo mais silenciosos. Como o primeiro pico est√° em torno de 620 Hz, este √© o espectro de frequ√™ncia de uma nota E‚ô≠ (Mi bemol).

O resultado da DFT √© um array de n√∫meros complexos, compostos de componentes reais e imagin√°rios. Obtendo a magnitude (valor absoluto, sem sinal) com `np.abs(dft)`, extra√≠mos as informa√ß√£o de amplitude do espectrograma. O √¢ngulo entre os componentes reais e imagin√°rios fornece o chamado espectro de fase, mas isso √© frequentemente descartado em aplica√ß√µes de machine learning.

Voc√™ usou `librosa.amplitude_to_db()` para converter os valores de amplitude para a escala de decib√©is, facilitando a visualiza√ß√£o dos m√≠nimos detalhes no espectro. √Äs vezes, as pessoas usam o **espectro de pot√™ncia**, que mede a energia em vez da amplitude; isso √© simplesmente um espectro com os valores de amplitude ao quadrado.

<Tip>
üí° Na pr√°tica, as pessoas usam o termo FFT e DFT como sin√¥nimos, j√° que a FFT, ou Fast Fouriter Transform (Transformada R√°pida de Fourier), √© a √∫nica maneira eficiente de calcular a DFT em um computador.
</Tip>

O espectro de frequ√™ncia de um sinal de √°udio cont√©m exatamente as mesmas informa√ß√µes que sua forma de onda ‚Äî s√£o simplesmente duas maneiras diferentes de olhar para os mesmos dados (no caso, as primeiras 4096 amostras do som de trompete). Enquanto a forma de onda tra√ßa a amplitude do sinal de √°udio ao longo do tempo, o espectro visualiza as amplitudes de cada frequ√™ncia em um ponto fixo no tempo.

## Espectrograma

E se quisermos ver como as frequ√™ncias em um sinal de √°udio mudam? O trompete toca v√°rias notas e todas elas t√™m frequ√™ncias diferentes. O problema √© que o espectro mostra apenas "uma foto" das frequ√™ncias em um dado instante. A solu√ß√£o √© fazer v√°rias DFTs, cada uma cobrindo apenas uma pequena fatia de tempo, e empilhar os espectros resultantes formando um **espectrograma**.

Um espectrograma mostra o conte√∫do da frequ√™ncia de um sinal de √°udio √† ao longo do tempo. Ele permite que voc√™ veja tempo, frequ√™ncia e amplitude tudo em um √∫nico gr√°fico. O algoritmo que realiza esse c√°lculo √© o STFT (Short Time Fourier Transform) ou Transformada de Fourier de Tempo Curto.

O espectrograma √© uma das ferramentas de √°udio mais informativas dispon√≠veis para voc√™. Por exemplo, ao trabalhar com uma grava√ß√£o de m√∫sica, voc√™ pode ver os v√°rios instrumentos e faixas vocais e como eles contribuem para o som geral. Na fala, voc√™ pode identificar diferentes sons de vogais, pois cada vogal √© caracterizada por frequ√™ncias particulares.

Vamos plotar um espectrograma para o mesmo som de trompete, usando as fun√ß√µes `stft()` e `specshow()` da librosa:

```py
import numpy as np

D = librosa.stft(array)
S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)

plt.figure().set_figwidth(12)
librosa.display.specshow(S_db, x_axis="time", y_axis="hz")
plt.colorbar()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/spectrogram_plot.png" alt="Spectrogram plot">
</div>

Neste gr√°fico, o eixo x representa o tempo como na visualiza√ß√£o da forma de onda, mas agora o eixo y representa a frequ√™ncia em Hz. A intensidade da cor d√° a amplitude ou pot√™ncia do componente de frequ√™ncia em cada ponto no tempo, medida em decib√©is (dB).

O espectrograma √© criado obtendo trechos curtos do sinal de √°udio, geralmente durando alguns milissegundos, e calculando a transformada de Fourier discreta de cada trecho para obter seu espectro de frequ√™ncia. Os espectros resultantes s√£o colocados em sequ√™ncia eixo do tempo para criar o espectrograma. Cada fatia vertical nesta imagem corresponde a um √∫nico espectro de frequ√™ncia, como se o gr√°fico fosse girado. Por padr√£o, `librosa.stft()` divide o sinal de √°udio em trechos de 2048 amostras, o que d√° um bom equil√≠brio entre resolu√ß√£o de frequ√™ncia e resolu√ß√£o de tempo.

Como o espectrograma e a forma de onda s√£o visualiza√ß√µes diferentes dos mesmos dados, √© poss√≠vel transformar o espectrograma de volta na forma de onda original usando a STFT inversa. No entanto, isso requer a informa√ß√£o de fase, al√©m da amplitude. Se o espectograma foi gerado por um modelo de machine learning, geralmente ele cont√©m s√≥ as amplitudes. Neste caso, n√≥s podemos usar um algoritmo de reconstru√ß√£o de fase como o cl√°ssico algoritmo de Griffin-Lim ou usar uma rede neural chamada vocoder, que reconstroi a forma de onda partir do espectograma.

Os espectrogramas n√£o s√£o usados apenas para visualiza√ß√£o. Muitos modelos de machine learning utilizam espectrogramas como entrada ‚Äî ao inv√©s de formas de onda ‚Äî e produzem espectrogramas como sa√≠da.

Agora que sabemos o que √© um espectrograma e como √© feito, vamos dar uma olhada em uma variante dele amplamente usada para o processamento de fala: o espectrograma mel.

## Espectrograma Mel

Um espectrograma mel √© uma varia√ß√£o do espectrograma que √© comumente usada no processamento de fala e tarefas de machine learning.
√â semelhante a um espectrograma no sentido de que mostra o conte√∫do de frequ√™ncia de um sinal de √°udio ao longo do tempo, mas em um eixo de frequ√™ncia diferente.

Em um espectrograma padr√£o, o eixo de frequ√™ncia √© linear e √© medido em hertz (Hz). No entanto, o sistema auditivo humano
√© mais sens√≠vel a mudan√ßas em frequ√™ncias mais baixas do que em frequ√™ncias mais altas, e essa sensibilidade diminui logaritmicamente
√† medida que a frequ√™ncia aumenta. A escala mel √© uma escala perceptual que aproxima a resposta de frequ√™ncia n√£o linear do ouvido humano.

Para criar um espectrograma mel, a STFT √© usada como antes, dividindo o √°udio em segmentos curtos para obter uma sequ√™ncia
de espectros de frequ√™ncia. Al√©m disso, cada espectro √© enviado atrav√©s de um conjunto de filtros, o chamado banco de filtros mel, para
transformar as frequ√™ncias para a escala mel.

Vamos ver como podemos plotar um espectrograma mel usando a fun√ß√£o `melspectrogram()` da librosa, que realiza todos esses passos para n√≥s:

```py
S = librosa.feature.melspectrogram(y=array, sr=sampling_rate, n_mels=128, fmax=8000)
S_dB = librosa.power_to_db(S, ref=np.max)

plt.figure().set_figwidth(12)
librosa.display.specshow(S_dB, x_axis="time", y_axis="mel", sr=sampling_rate, fmax=8000)
plt.colorbar()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/mel-spectrogram.png" alt="Gr√°fico de espectrograma mel">
</div>


No exemplo acima, `n_mels` indica o n√∫mero de bandas mel a serem geradas. As bandas mel definem um conjunto de
faixas de frequ√™ncia que dividem o espectro em componentes perceptualmente significativos, usando um conjunto de filtros cuja forma e espa√ßamento
s√£o escolhidos para imitar a forma como o ouvido humano responde a diferentes frequ√™ncias. Valores comuns para `n_mels` s√£o 40 ou 80. `fmax`
indica a frequ√™ncia mais alta (em Hz) com a qual nos preocupamos.

Assim como com um espectrograma regular, √© pr√°tica comum expressar a intesidade de cada frequ√™ncia mel em
decib√©is. Isso √© comumente referido como um **espectrograma log-mel**, porque a convers√£o para decib√©is envolve uma
opera√ß√£o logar√≠tmica. O exemplo acima usou `librosa.power_to_db()` j√° que `librosa.feature.melspectrogram()` cria um espectrograma de pot√™ncia.

<Tip>
üí° Nem todos os espectrogramas mel s√£o iguais! Existem duas escalas mel diferentes comumente usadas ("htk" e "slaney"),
e, ao inv√©s do espectrograma de pot√™ncia, pode ser usado o espectrograma de amplitude.

 A convers√£o para um espectrograma log-mel n√£o
calcula sempre decib√©is reais, mas pode simplesmente calcular `log`. Portanto, se um modelo de aprendizado de m√°quina espera um espectrograma mel
como entrada, certifique-se de garantir que voc√™ est√° calculando da mesma maneira.
</Tip>

Criar um espectrograma mel √© uma opera√ß√£o com perda, pois envolve filtragem do sinal. Converter um espectrograma mel de volta
em uma forma de onda √© mais dif√≠cil do que fazer isso para um espectrograma regular, pois exige a estima√ß√£o das frequ√™ncias
que foram descartadas. √â por isso que modelos de aprendizado de m√°quina como o vocoder HiFiGAN s√£o necess√°rios para produzir uma forma de onda a partir de um espectrograma mel.

Comparado a um espectrograma padr√£o, um espectrograma mel pode capturar caracter√≠sticas mais significativas do sinal de √°udio para
a percep√ß√£o humana, tornando-o uma escolha popular em tarefas como reconhecimento de fala, identifica√ß√£o de falante e classifica√ß√£o de g√™nero musical.

Agora que voc√™ sabe como visualizar exemplos de dados de √°udio, v√° em frente e tente ver como seus sons favoritos parecem. :)
