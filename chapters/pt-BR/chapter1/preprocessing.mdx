# Pr√©-processamento de um dataset de √°udio

Carregar um dataset com ü§ó Datasets √© parte da brincadeira. Se voc√™ planeja us√°-lo para treinar um modelo, ou para executar infer√™ncia, voc√™ precisar√° pr√©-processar os dados primeiro. Geralmente, isso se resume nas seguintes etapas:

* Reamostragem (resampling) dos dados de √°udio
* Filtragem do dataset
* Convers√£o dos dados de √°udio para a formato esperado pelo modelo

## Reamostragem dos dados de √°udio

A fun√ß√£o `load_dataset` baixa exemplos de √°udio com a taxa de amostragem na qual foram publicados. Esta n√£o √© sempre a taxa de amostragem esperada por um modelo que voc√™ planeja treinar, ou usar para infer√™ncia. Se houver uma diferen√ßa entre as taxas de amostragem, voc√™ pode reamostrar (resample) o √°udio para a taxa de amostragem esperada pelo modelo.

A maioria dos modelos pr√©-treinados dispon√≠veis foram treinados em datasets de √°udio com uma taxa de amostragem de 16 kHz. Quando exploramos o dataset MINDS-14, voc√™ pode ter notado que ele √© amostrado a 8 kHz, o que significa que provavelmente precisaremos aumentar a amostragem.

Para fazer isso, use o m√©todo `cast_column` de ü§ó Datasets. Esta opera√ß√£o n√£o altera o arquivo de √°udio baixado, mas indica aos datasets para reamostrar os exemplos de √°udio sob demanda a medida que forem carregados. O seguinte c√≥digo definir√° a taxa de amostragem para 16kHz:

```py
from datasets import Audio

minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
```

Recarregue o primeiro exemplo de √°udio no dataset MINDS-14, e verifique se ele foi reamostrado para a `taxa de amostragem` desejada:

```py
minds[0]
```

**Sa√≠da:**
```out
{
    "path": "/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-AU~PAY_BILL/response_4.wav",
    "audio": {
        "path": "/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-AU~PAY_BILL/response_4.wav",
        "array": array(
            [
                2.0634243e-05,
                1.9437837e-04,
                2.2419340e-04,
                ...,
                9.3852862e-04,
                1.1302452e-03,
                7.1531429e-04,
            ],
            dtype=float32,
        ),
        "sampling_rate": 16000,
    },
    "transcription": "I would like to pay my electricity bill using my card can you please assist",
    "intent_class": 13,
}
```

Repare que os valores do array tamb√©m est√£o diferentes agora. Isso ocorre porque agora temos o dobro do n√∫mero de valores de amplitude para cada um que t√≠nhamos antes.

<Tip>
üí° Algumas informa√ß√µes sobre a reamostragem (resampling): Se um sinal de √°udio foi amostrado a 8 kHz, de modo que possui 8000 amostras coletadas por segundo, sabemos que o √°udio n√£o cont√©m frequ√™ncias acima de 4 kHz. Isso √© garantido pelo teorema de Nyquist. Com base nisso, podemos assumir que entre as amostras de um sinal cont√≠nuo, especificamente os pontos que existem entre elas, e n√£o foram coletados, devido a amostragem, formam uma curva suave. Ent√£o, aumentar a taxa de amostragem para um valor mais alto √© apenas uma quest√£o de calcular valores de amostra adicionais que v√£o entre os existentes, formando essa curva suave. No entanto, diminuir a amostragem (downsampling) requer primeiro filtrar quaisquer frequ√™ncias que seriam maiores que o novo limite de Nyquist, antes de estimar os novos pontos de amostra (pois diminuindo a amostragem, a frequ√™ncia m√°xima tamb√©m diminuiria). Em outras palavras, voc√™ n√£o pode diminuir a amostragem por um fator de 2x simplesmente descartando todas as outras amostras - isso criar√° distor√ß√µes no sinal chamadas de alias. Fazer a reamostragem corretamente √© complicado e √© melhor deixar para bibliotecas bem testadas como librosa ou ü§ó Datasets.
</Tip>

## Filtragem do dataset

Talvez voc√™ precise filtrar os dados com base em alguns crit√©rios. Um dos casos comuns envolve limitar os exemplos de √°udio a uma certa dura√ß√£o. Por exemplo, podemos querer ignorar quaisquer exemplos mais longos que 20s para prevenir erros de falta de mem√≥ria ao treinar um modelo.

Podemos fazer isso usando o m√©todo `filter` da ü§ó Datasets e passando uma fun√ß√£o com a l√≥gica de filtragem para ele. Vamos come√ßar escrevendo uma fun√ß√£o que indica quais exemplos manter e quais descartar. Esta fun√ß√£o, `is_audio_length_in_range`, retorna `True` se a dura√ß√£o for menor 20s, e `False` se for maior.

```py
MAX_DURATION_IN_SECONDS = 20.0


def is_audio_length_in_range(input_length):
    return input_length < MAX_DURATION_IN_SECONDS
```

A fun√ß√£o de filtragem pode ser aplicada a uma coluna do dataset, mas n√£o temos uma coluna com a dura√ß√£o da do √°udio neste dataset. No entanto, podemos criar uma, filtrar com base nos valores dessa coluna e, em seguida, remov√™-la.

```py
# use librosa para obter a dura√ß√£o do √°udio
new_column = [librosa.get_duration(path=x) for x in minds["path"]]
minds = minds.add_column("duration", new_column)

# use o m√©todo `filter` de ü§ó Datasets para aplicar a fun√ß√£o de filtragem
minds = minds.filter(is_audio_length_in_range, input_columns=["duration"])

# remova a coluna auxiliar tempor√°ria
minds = minds.remove_columns(["duration"])
minds
```

**Sa√≠da:**
```out
Dataset({features: ["path", "audio", "transcription", "intent_class"], num_rows: 624})
```

Perceba que o dataset foi filtrado de 654 exemplos para 624 (num_rows).

## Pr√©-processamento dos dados de √°udio

Um dos aspectos mais desafiadores de trabalhar com datasets de √°udio √© preparar os dados no formato correto para o treinamento do modelo. Como voc√™ viu, os dados de √°udio brutos v√™m como um array de valores de amostras (do sinal original). No entanto, modelos pr√©-treinados, seja para us√°-los em infer√™ncia, seja para ajust√°-los (fine tuning) ao seu projeto, esperam que os dados brutos sejam convertidos em caracter√≠sticas (features) de entrada. Os requisitos para as caracter√≠sticas de entrada podem variar de um modelo para outro - eles dependem da arquitetura do modelo e dos dados com os quais foi pr√©-treinado. A boa not√≠cia √© que, para cada modelo de √°udio suportado, ü§ó Transformers oferece uma classe de extrator de caracter√≠sticas (feature extractor) que pode converter dados de √°udio brutos nas caracter√≠sticas de entrada que o modelo espera.

Ent√£o, o que um extrator de caracter√≠sticas faz com os dados de √°udio brutos? Vamos dar uma olhada extrator de caracter√≠sticas chamado [Whisper](https://huggingface.co/papers/2212.04356) para entender algumas transforma√ß√µes comuns de extra√ß√£o de caracter√≠sticas. Whisper √© um modelo pr√©-treinado para reconhecimento autom√°tico de fala (ASR) publicado em setembro de 2022 por Alec Radford et al. da OpenAI.

Primeiro, o extrator de caracter√≠sticas do Whisper preenche/corta um grupo (batch) de exemplos de √°udio de forma que todos os exemplos tenham uma dura√ß√£o de 30s. Exemplos mais curtos que isso s√£o complementados com zeros no final, at√© formar 30s de dura√ß√£o (zeros em um sinal de √°udio correspondem a sil√™ncio ou nada de sinal). Exemplos que tem mais de 30s s√£o cortados nos 30s. Uma vez que todos os elementos no grupo s√£o padronizados para uma dura√ß√£o fixa, n√£o h√° necessidade de uma m√°scara de aten√ß√£o (attention mask). O Whisper √© √∫nico neste aspecto, a maioria dos outros modelos de √°udio requer uma m√°scara de aten√ß√£o que detalha onde as sequ√™ncias foram preenchidas, e assim onde elas devem ser ignoradas no mecanismo de auto-aten√ß√£o (self-attention). O Whisper √© treinado para operar sem uma m√°scara de aten√ß√£o e inferir do pr√≥prio sinal onde ele deve ignorar.

A segunda opera√ß√£o que o extrator de caracter√≠sticas do Whisper realiza √© converter os arrays de √°udio padronizados em espectrogramas log-mel. Lembre que esses espectrogramas descrevem como as frequ√™ncias de um sinal mudam ao longo do tempo, expressas na escala mel e medidas em decib√©is (a parte do log) para tornar as frequ√™ncias e amplitudes mais representativas da audi√ß√£o humana.

Todas essas transforma√ß√µes podem ser aplicadas aos seus dados de √°udio brutos com algumas poucas linhas de c√≥digo. Vamos colocar a m√£o na massa e carregar o extrator de caracter√≠sticas a partir do checkpoint pr√©-treinado do Whisper e deix√°-lo pronto para usar nos nossos dados de √°udio:

```py
# Instale com o comando: pip install transformers
# Voc√™ pode acessar a p√°gina incial dos Transformers para instru√ß√µes de instala√ß√£o!
from transformers import WhisperFeatureExtractor

feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-small")
```

Ent√£o, voc√™ pode escrever uma fun√ß√£o para pr√©-processar um exemplo de √°udio, passando ele para o `feature_extractor`.

```py
def prepare_dataset(example):
    audio = example["audio"]
    features = feature_extractor(
        audio["array"], sampling_rate=audio["sampling_rate"], padding=True
    )
    return features
```

Podemos aplicar a fun√ß√£o de prepara√ß√£o de dados a todos os nossos exemplos de treinamento usando o m√©todo `map` de ü§ó Datasets:

```py
minds = minds.map(prepare_dataset)
minds
```

**Sa√≠da:**
```out
Dataset(
    {
        features: ["path", "audio", "transcription", "intent_class", "input_features"],
        num_rows: 624,
    }
)
```
Sem muito esfor√ßo, agora temos espectrogramas log-mel no campo `input_features` do dataset.

Vamos visualiz√°-lo em dos exemplos do dataset `minds`:

```py
import numpy as np

example = minds[0]
input_features = example["input_features"]

plt.figure().set_figwidth(12)
librosa.display.specshow(
    np.asarray(input_features[0]),
    x_axis="time",
    y_axis="mel",
    sr=feature_extractor.sampling_rate,
    hop_length=feature_extractor.hop_length,
)
plt.colorbar()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/log_mel_whisper.png" alt="Gr√°fico de espectrograma log mel">
</div>

Agora voc√™ pode ver como fica um input de √°udio para o modelo Whisper ap√≥s o pr√©-processamento.

A classe de extrator de caracter√≠sticas do modelo cuida de transformar os dados de √°udio brutos para o formato que o modelo espera. No entanto, muitas tarefas envolvendo √°udio s√£o multimodais, como, por exemplo, o reconhecimento de fala. Nestes casos, ü§ó Transformers tamb√©m oferece tokenizadores (tokenizers) espec√≠ficos do modelo para processar as entradas de texto. Para se aprofundar em tokenizadores, consulte nosso [curso de NLP](https://huggingface.co/course/chapter2/4).

Voc√™ pode carregar o extrator de caracter√≠sticas e o tokenizador do Whisper (e outros modelos multimodais) separadamente, ou voc√™ pode carregar ambos por meio de algo chamado de processor (processador). Para simplificar ainda mais, use `AutoProcessor` para carregar o extrator de caracter√≠sticas e processador de um modelo a partir de um checkpoint, assim:

```py
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("openai/whisper-small")
```

Aqui mostramos os passos fundamentais de prepara√ß√£o dos dados. Claro, dados personalizados podem exigir pr√©-processamento mais complexo. Neste caso, voc√™ pode estender a fun√ß√£o `prepare_dataset` para realizar qualquer tipo de transforma√ß√£o de dados personalizada. Com ü§ó Datasets, se voc√™ pode escrever sua personaliza√ß√£o como uma fun√ß√£o Python, ent√£o voc√™ pode [aplic√°-la](https://huggingface.co/docs/datasets/audio_process) ao seu dataset!
