# Pre-trained models for audio classification

The Hugging Face Hub is home to over 500 pre-trained models for audio classification. In this section, we'll go through
some of the most common audio classification tasks and suggest appropriate pre-trained models for each. Using the `pipeline()`
class, switching between models and tasks is straightforward - once you know how to use `pipeline()` for one model, you'll
be able to use it for any model on the Hub no code changes! This makes experimenting with the `pipeline()` class extremely
fast, allowing you to quickly select the best pre-trained model for your needs.

Before we jump into the various audio classification problems, let's quickly recap the transformer architectures typically
used. The standard audio classification architecture is motivated by the nature of the task; we want to transform a sequence
of audio inputs (i.e. our input audio array) into a single class label prediction. Hence, there is a preference
for _encoder-only_ models. Encoder-only models first map the input audio sequence into a sequence of hidden-state
representations by passing the inputs through a transformer block. The sequence of hidden-state representations is then
mapped to a class label output by taking the mean over the hidden-states, and passing the resulting vector through a linear
classification layer. TODO(SG): re-use a diagram from Unit3.

Decoder-only models introduce unnecessary complexity to the task, since they assume that the outputs can also be a sequence
(rather than a single class label prediction) and so generate multiple outputs. Therefore, they have slower inference speed
and tend not to be used. Encoder-decoder models are largely omitted for the same reason. These architecture choices are
analogous to those in NLP, where encoder-only models such as BERT are favoured for sequence classification tasks, and
decoder-only models such as GPT reserved for sequence generation tasks.

Now that we've recapped the standard transformer architecture for audio classification, let's jump into the different
subsets of audio classification and cover the most popular models.

## Keyword Spotting
Keyword spotting (KWS) is the task of identifying a keyword in a spoken utterance. The set of possible keywords forms the
set of predicted class labels. Hence, to use a pre-trained keyword spotting model, you should ensure that your keywords
match those that the model was pre-trained on. Below, we'll introduce two datasets and models for keyword spotting.

### Minds-14

TODO(SG): more exciting dataset?

Let's go ahead and use the same [MINDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset that you have explored
in the previous unit. If you recall, MINDS-14 contains recordings of people asking an e-banking system questions in several
languages and dialects, and has the `intent_class` for each recording. We can classify the recordings by intent of the call.

```python
from datasets import load_dataset

minds = load_dataset("PolyAI/minds14", name="en-AU", split="train")
```

We'll load the checkpoint [`"anton-l/xtreme_s_xlsr_300m_minds14"`](https://huggingface.co/anton-l/xtreme_s_xlsr_300m_minds14),
which is an XLS-R model fine-tuned on MINDS-14 for approximately 50 epochs. It achieves 90% accuracy over all languages
from MINDS-14 on the evaluation set.

```python
from transformers import pipeline

classifier = pipeline(
    "audio-classification",
    model="anton-l/xtreme_s_xlsr_300m_minds14",
)
```

Finally, we can pass a sample to the classification pipeline to make a prediction:
```python
classifier(minds[0]["path"])
```
**Output:**
```
[
    {"score": 0.9631525278091431, "label": "pay_bill"},
    {"score": 0.02819698303937912, "label": "freeze"},
    {"score": 0.0032787492964416742, "label": "card_issues"},
    {"score": 0.0019414445850998163, "label": "abroad"},
    {"score": 0.0008378693601116538, "label": "high_value_payment"},
]
```
Great! We've identified that the intent of the call was paying a bill, with probability 96%.

### SUPERB

```python
superb = load_dataset("anton-l/superb_demo", "ks", split="test")

classifier = pipeline("audio-classification", model="superb/hubert-base-superb-ks")
classifier(superb[-1]["file"])
```
**Output:**
```
[{'score': 0.9992257356643677, 'label': 'no'},
 {'score': 0.0004319172876421362, 'label': 'go'},
 {'score': 0.0003331181069370359, 'label': 'down'},
 {'score': 8.870103556546383e-06, 'label': '_unknown_'},
 {'score': 2.6367405325800064e-07, 'label': 'stop'}]
```


## Language Identification

```python
classifier = pipeline("audio-classification", model="sanchit-gandhi/whisper-medium-fleurs-lang-id")

ds = load_dataset("google/fleurs", "all", split="validation", streaming=True)
sample = next(iter(ds))

classifier(sample["audio"])
```
**Output:**
```
[{'score': 0.9999330043792725, 'label': 'Afrikaans'},
 {'score': 7.093023668858223e-06, 'label': 'Northern-Sotho'},
 {'score': 4.269149485480739e-06, 'label': 'Icelandic'},
 {'score': 3.2661141631251667e-06, 'label': 'Danish'},
 {'score': 3.2580724109720904e-06, 'label': 'Cantonese Chinese'}]
```