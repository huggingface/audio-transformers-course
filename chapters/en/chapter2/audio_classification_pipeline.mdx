# Audio classification with a pipeline

Audio classification involves assigning one or more labels to an audio recording based on its content. The labels
could correspond to different sound categories, such as music, speech, or noise, or more specific categories like
bird song or car engine sounds. We can even go so far as labelling a music sample based on its genre, which we'll see more
of in Unit 4.

Before diving into details on how the most popular audio transformers work, and before fine-tuning a custom model, let's
see how you can use an off-the-shelf pre-trained model for audio classification with only a few lines of code with ðŸ¤— Transformers.

Let's go ahead and use the same [MINDS-14](https://huggingface.co/datasets/PolyAI/minds14) dataset that you have explored
in the previous unit. If you recall, MINDS-14 contains recordings of people asking an e-banking system questions in several
languages and dialects, and has the `intent_class` for each recording. We can classify the recordings by intent of the call.
Such a system could be used as the first stage of an automated call-centre, to put a customer through to the correct department
based on what they've said.

Just as before, let's start by loading the `en-AU` subset of the data to try out the pipeline:

```py
from datasets import load_dataset

minds = load_dataset("PolyAI/minds14", name="en-AU", split="train")
```

To classify an audio recording into a set of classes, we can use the `audio-classification` pipeline from ðŸ¤— Transformers.
In our case, we need a model that's been fine-tuned for intent classification, and specifically on
the MINDS-14 dataset. Luckily for us, the Hub has a [model](https://huggingface.co/anton-l/xtreme_s_xlsr_300m_minds14)
that does just that! Let's load it by using the `pipeline()` function:

```py
from transformers import pipeline

classifier = pipeline(
    "audio-classification",
    model="anton-l/xtreme_s_xlsr_300m_minds14",
)
```

All the preprocessing of the raw audio data will be conveniently handled for us by the pipeline, including any resampling.
Let's pick an example to try it out:

```py
example = minds[0]
```

If you recall the structure of the dataset, the audio data is stored as a dictionary array under the key `["audio"]`:

```python
print(example["audio"])
```

The resulting dictionary has two further keys:
* `["array"]`: the 1-dimensional audio array
* `["sampling_rate"]`: the sampling rate of the audio sample

This is exactly the format of input data that the pipeline expects. Thus, we can pass the `["audio"]` sample straight
to the `classifier`:

```py
classifier(example["audio"])
[
    {"score": 0.9631525278091431, "label": "pay_bill"},
    {"score": 0.02819698303937912, "label": "freeze"},
    {"score": 0.0032787492964416742, "label": "card_issues"},
    {"score": 0.0019414445850998163, "label": "abroad"},
    {"score": 0.0008378693601116538, "label": "high_value_payment"},
]
```

The model is very confident that the caller intended to learn about paying their bill. Let's see what the actual label for
this example is:

```py
id2label = minds.features["intent_class"].int2str
id2label(example["intent_class"])
"pay_bill"
```

Hooray! The predicted label was correct! Here we were lucky to find a model that can classify the exact labels that we need.
A lot of the time, when dealing with a classification task, a pre-trained model's set of classes is not exactly the same
as the classes you need the model to distinguish. In this case, you can fine-tune a pre-trained model to "calibrate" it to
your exact set of class labels. We'll learn how to do this in the upcoming units. Now, let's take a look at another very
common task in speech processing, _automatic speech recognition_.
