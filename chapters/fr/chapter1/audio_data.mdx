# Introduction aux donn√©es audio

Le **taux d'√©chantillonnage** (√©galement appel√© fr√©quence d'√©chantillonnage) est le nombre d'√©chantillons pr√©lev√©s en une seconde et est mesur√© en hertz (Hz). 
Pour vous donner un point de r√©f√©rence, l'audio de qualit√© CD a un taux d'√©chantillonnage de 44 100 Hz, ce qui signifie que les √©chantillons sont pr√©lev√©s 44 100 fois par seconde. √Ä titre de comparaison, l'audio haute r√©solution a un taux d'√©chantillonnage de 192 000 Hz ou 192 kHz. 
Un taux d'√©chantillonnage couramment utilis√© dans les mod√®les vocaux d'apprentissage est de 16 000 Hz ou 16 kHz.  

Le choix de la fr√©quence d'√©chantillonnage d√©termine principalement la fr√©quence la plus √©lev√©e qui peut √™tre captur√©e √† partir du signal. 
Ceci est √©galement connu sous le nom de limite de Nyquist et correspond exactement √† la moiti√© du taux d'√©chantillonnage. 
Les fr√©quences audibles dans la parole humaine sont inf√©rieures √† 8 kHz et, par cons√©quent, l'√©chantillonnage de la parole √† 16 kHz est suffisant.
L'utilisation d'un taux d'√©chantillonnage plus √©lev√© ne permettra pas de capturer plus d'informations et ne fera qu'augmenter le co√ªt de calcul du traitement de ces fichiers. 
D'autre part, l'√©chantillonnage audio √† un taux d'√©chantillonnage trop faible entra√Ænera une perte d'informations. 
La parole √©chantillonn√©e √† 8 kHz sonnera √©touff√©e car les fr√©quences plus √©lev√©es ne peuvent pas √™tre captur√©es √† ce rythme.  

Il est important de vous assurer que tous les exemples audio de votre jeu de donn√©es ont le m√™me taux d'√©chantillonnage lorsque vous travaillez sur une t√¢che audio.
Si vous pr√©voyez d'utiliser des donn√©es audio personnalis√©es pour finetuner un mod√®le pr√©-entra√Æn√©, le taux d'√©chantillonnage de vos donn√©es doit correspondre au taux d'√©chantillonnage des donn√©es sur lesquelles le mod√®le a √©t√© pr√©-entra√Æn√©. 
La fr√©quence d'√©chantillonnage d√©termine l'intervalle de temps entre les √©chantillons audio successifs, ce qui a un impact sur la r√©solution temporelle des donn√©es audio. 
Prenons un exemple : un son de 5 secondes √† une fr√©quence d'√©chantillonnage de 16 000 Hz sera repr√©sent√© comme une s√©rie de 80 000 valeurs, tandis que le m√™me son de 5 secondes √† une fr√©quence d'√©chantillonnage de 8 000 Hz sera repr√©sent√© comme une s√©rie de 40 000 valeurs. 
Les *transformers* qui r√©solvent les t√¢ches audio traitent les exemples comme des s√©quences et s'appuient sur des m√©canismes d'attention pour apprendre l'audio ou la repr√©sentation multimodale. 
√âtant donn√© que les s√©quences sont diff√©rentes pour les exemples audio √† des taux d'√©chantillonnage diff√©rents, il sera difficile pour les mod√®les de g√©n√©raliser entre les taux d'√©chantillonnage.
**Le r√©√©chantillonnage** est le processus de mise en correspondance des taux d'√©chantillonnage et fait partie du [pr√©traitement](preprocessing#resampling-the-audio-data) des donn√©es audio.

## Amplitude et profondeur de bits

Bien que le taux d'√©chantillonnage vous indique √† quelle fr√©quence les √©chantillons sont pr√©lev√©s, quelles sont exactement les valeurs de chaque √©chantillon?
Le son est produit par des changements de pression atmosph√©rique √† des fr√©quences audibles pour les humains. 
L'**amplitude** d'un son d√©crit le niveau de pression acoustique √† un instant donn√© et est mesur√©e en d√©cibels (dB). 
Nous percevons l'amplitude comme un volume sonore. Pour vous donner un exemple, une voix normale est inf√©rieure √† 60 dB, et un concert de rock peut √™tre autour de 125 dB, repoussant les limites de l'audition humaine.
En audio, chaque √©chantillon enregistre l'amplitude de l'onde √† un moment donn√©. 
La **profondeur de bits** de l'√©chantillon d√©termine avec quelle pr√©cision cette valeur d'amplitude peut √™tre d√©crite. 
Plus la profondeur de bits est √©lev√©e, plus la repr√©sentation num√©rique se rapproche fid√®lement de l'onde sonore continue d'origine.
Les profondeurs de bits audio les plus courantes sont 16 bits et 24 bits. 
Chacun est un terme binaire, repr√©sentant le nombre d'√©tapes possibles auxquelles la valeur d'amplitude peut √™tre quantifi√©e lorsqu'elle est convertie de continue √† discr√®te: 65 536 √©tapes pour l'audio 16 bits, 16 777 216 √©tapes pour l'audio 24 bits. 
√âtant donn√© que la quantification implique d'arrondir la valeur continue √† une valeur discr√®te, le processus d'√©chantillonnage introduit du bruit. Plus la profondeur de bits est √©lev√©e, plus ce bruit de quantification est faible. 
En pratique, le bruit de quantification de l'audio 16 bits est d√©j√† suffisamment faible pour √™tre inaudible et l'utilisation de profondeurs de bits plus √©lev√©es n'est g√©n√©ralement pas n√©cessaire.
Vous pouvez √©galement rencontrer de l'audio 32 bits. 
Cela stocke les √©chantillons sous forme de valeurs √† virgule flottante, tandis que l'audio 16 bits et 24 bits utilise des √©chantillons entiers. 
La pr√©cision d'une valeur √† virgule flottante de 32 bits est de 24 bits, ce qui lui donne la m√™me profondeur de bits que l'audio 24 bits.
Les √©chantillons audio en virgule flottante devraient se situer dans la plage [-1.0, 1.0]. 
√âtant donn√© que les mod√®les d'apprentissage automatique fonctionnent naturellement sur des donn√©es en virgule flottante, l'audio doit d'abord √™tre converti au format √† virgule flottante avant de pouvoir √™tre utilis√© pour entra√Æner le mod√®le. 
Nous verrons comment faire cela dans la section suivante sur le [Pr√©traitement] (preprocessing).
Tout comme pour les signaux audio continus, l'amplitude de l'audio num√©rique est g√©n√©ralement exprim√©e en d√©cibels (dB). 
L'audition humaine √©tant de nature logarithmique - nos oreilles sont plus sensibles aux petites fluctuations des sons calmes qu'√† celles des sons forts - l'intensit√© d'un son est plus facile √† interpr√©ter si les amplitudes sont exprim√©es en d√©cibels, qui sont √©galement logarithmiques.
L'√©chelle de d√©cibels pour l'audio r√©el commence √† 0 dB, ce qui repr√©sente le son le plus faible possible que les humains peuvent entendre, et les sons plus forts ont des valeurs plus importantes. 
Cependant, pour les signaux audio num√©riques, 0 dB est l'amplitude la plus forte possible, tandis que toutes les autres amplitudes sont n√©gatives. 
En r√®gle g√©n√©rale: chaque -6 dB est une r√©duction de moiti√© de l'amplitude, et tout ce qui est inf√©rieur √† -60 dB est g√©n√©ralement inaudible √† moins que vous n'augmentiez vraiment le volume.

## L'audio comme forme d'onde

Vous avez peut-√™tre vu des sons visualis√©s sous la **forme d'onde** qui tra√ßant les valeurs de l'√©chantillon au fil du temps et illustrant les changements d'amplitude du son. Ceci est aussi connu sous le nom de repr√©sentation du *domaine temporel* du son.
Ce type de visualisation est utile pour identifier des caract√©ristiques sp√©cifiques du signal audio telles que la synchronisation des √©v√©nements sonores individuels, l'intensit√© sonore globale du signal et toute irr√©gularit√© ou bruit pr√©sent dans l'audio.

Pour tracer la forme d'onde d'un signal audio, nous pouvons utiliser une biblioth√®que Python `librosa`:

```bash
pip install librosa
```

Prenons un exemple de son appel√© ¬´ trompette ¬ª qui vient avec la biblioth√®que:

```py
import librosa

array, sampling_rate = librosa.load(librosa.ex("trumpet"))
```

L'exemple est charg√© sous la forme d'un tuple de s√©ries temporelles audio (ici nous l'appelons `array`) et de taux d'√©chantillonnage (`sampling_rate`).
Jetons un coup d'≈ìil √† la forme d'onde de ce son en utilisant la fonction `waveshow()` de librosa:

```py
import matplotlib.pyplot as plt
import librosa.display

plt.figure().set_figwidth(12)
librosa.display.waveshow(array, sr=sampling_rate)
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/waveform_plot.png" alt="Waveform plot">
</div>

Cela trace l'amplitude du signal sur l'axe des y et le temps le long de l'axe des x. 
En d'autres termes, chaque point correspond √† une seule valeur d'√©chantillon qui a √©t√© prise lors de l'√©chantillonnage de ce son. 
Notez √©galement que librosa renvoie d√©j√† l'audio sous forme de valeurs √† virgule flottante et que les valeurs d'amplitude sont effectivement comprises dans la plage [-1.0, 1.0].
Visualiser l'audio et l'√©couter peut √™tre un outil utile pour comprendre les donn√©es avec lesquelles vous travaillez.
Vous pouvez voir la forme du signal, observer des mod√®les, apprendre √† rep√©rer le bruit ou la distorsion. 
Si vous pr√©traitez les donn√©es d'une mani√®re ou d'une autre, telle que la normalisation, le r√©√©chantillonnage ou le filtrage, vous pouvez confirmer visuellement que les √©tapes de pr√©traitement ont √©t√© appliqu√©es comme pr√©vu.
Apr√®s avoir entra√Æn√© un mod√®le, vous pouvez √©galement visualiser des exemples o√π des erreurs se produisent (par exemple, dans la t√¢che de classification audio) pour d√©boguer le probl√®me.

## Le spectre de fr√©quences

Une autre fa√ßon de visualiser les donn√©es audio consiste √† tracer le **spectre de fr√©quences** d'un signal audio, √©galement connu sous le nom de repr√©sentation du *domaine fr√©quentiel*. 
Le spectre est calcul√© √† l'aide de la transform√©e de Fourier discr√®te ou TFD. Il d√©crit les fr√©quences individuelles qui composent le signal et leur force.
Tra√ßons le spectre de fr√©quences pour le m√™me son de trompette en prenant la TFD en utilisant la fonction `rfft()` de numpy. Bien qu'il soit possible de tracer le spectre de l'ensemble du son, il est plus utile de regarder une petite r√©gion √† la place. 
Ici, nous allons prendre la TFD sur les 4096 premiers √©chantillons, ce qui correspond √† peu pr√®s √† la longueur de la premi√®re note jou√©e:

```py
import numpy as np

TFD_input = array[:4096]

# calculer la TDF
window = np.hanning(len(TFD_input))
windowed_input = TFD_input * window
TFD = np.fft.rfft(windowed_input)

# obtenir le spectre d'amplitude en d√©cibels
amplitude = np.abs(TFD)
amplitude_db = librosa.amplitude_to_db(amplitude, ref=np.max)

# obtenir les bacs de fr√©quence
frequency = librosa.fft_frequencies(sr=sampling_rate, n_fft=len(TFD_input))

plt.figure().set_figwidth(12)
plt.plot(frequency, amplitude_db)
plt.xlabel("Frequency (Hz)")
plt.ylabel("Amplitude (dB)")
plt.xscale("log")
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/spectrum_plot.png" alt="Spectrum plot">
</div>

Cela trace la force des diff√©rentes composantes de fr√©quence pr√©sentes dans ce segment audio. Les valeurs de fr√©quence sont sur l'axe des abscisses, g√©n√©ralement trac√©es sur une √©chelle logarithmique, tandis que leurs amplitudes sont sur l'axe des y.

Le spectre de fr√©quences que nous avons trac√© montre plusieurs pics. 
Ces pics correspondent aux harmoniques de la note jou√©e, les harmoniques sup√©rieures √©tant plus calmes. Puisque le premier pic est √† environ 620 Hz, c'est le spectre de fr√©quence d'une‚ô≠ note Mi.

La sortie de la TDF est un tableau de nombres complexes, compos√© de composants r√©els et imaginaires. 
Prendre la magnitude avec ` np.abs(TFD)`  extrait l'information d'amplitude du spectrogramme.
L'angle entre les composants r√©els et imaginaires fournit ce que l'on appelle le spectre de phase, mais il est souvent √©cart√© dans les applications d'apprentissage automatique.
Nous utilions `librosa.amplitude_to_db()` pour convertir les valeurs d'amplitude en √©chelle de d√©cibels, ce qui facilite la visualisation des d√©tails les plus fins du spectre. 
Parfois, les gens utilisent le **spectre de puissance**, qui mesure l'√©nergie plut√¥t que l'amplitude. Il s'agit simplement d'un spectre avec les valeurs d'amplitude au carr√©.

<Tip> 
üí° En pratique, les gens utilisent le terme FFT de mani√®re interchangeable avec TFD, car la FFT ou transform√©e de Fourier rapide est le seul moyen efficace de calculer la TFD sur un ordinateur.
</Tip>

Le spectre de fr√©quences d'un signal audio contient exactement la m√™me information que sa forme d'onde. Ce sont simplement deux fa√ßons diff√©rentes de regarder les m√™mes donn√©es (ici, les 4096 premiers √©chantillons du son de la trompette). 
L√† o√π la forme d'onde trace l'amplitude du signal audio au fil du temps, le spectre visualise les amplitudes des fr√©quences individuelles √† un moment donn√©.

## Spectrogramme

Et si nous voulons voir comment les fr√©quences d'un signal audio changent ? La trompette joue plusieurs notes et elles ont toutes des fr√©quences diff√©rentes. 
Le probl√®me est que le spectre ne montre qu'un instantan√© fig√© des fr√©quences √† un instant donn√©.
La solution consiste √† prendre plusieurs TFD, chacune ne couvrant qu'une petite tranche de temps, et √† empiler les spectres r√©sultants dans un **spectrogramme**.

Un spectrogramme trace le contenu en fr√©quence d'un signal audio au fil du temps. Il vous permet de voir le temps, la fr√©quence et l'amplitude sur un seul graphique. L'algorithme qui effectue ce calcul est la TFCT ou transform√©e de Fourier √† court terme.
Le spectrogramme est l'un des outils audio les plus informatifs √† notre disposition. 
Par exemple, lorsque nous travaillons avec un enregistrement musical, nous pouvons voir les diff√©rents instruments et pistes vocales et comment ils contribuent au son global. Dans la parole, nous pouvons identifier diff√©rents sons de voyelles car chaque voyelle est caract√©ris√©e par des fr√©quences particuli√®res.
Tra√ßons un spectrogramme pour le m√™me son de trompette, en utilisant les fonctions `stft()` et `specshow()` de librosa:

```py
import numpy as np

D = librosa.stft(array)
S_db = librosa.amplitude_to_db(np.abs(D), ref=np.max)

plt.figure().set_figwidth(12)
librosa.display.specshow(S_db, x_axis="time", y_axis="hz")
plt.colorbar()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/spectrogram_plot.png" alt="Spectrogram plot">
</div>

Dans ce graphique, l'axe des x repr√©sente le temps comme dans la visualisation de la forme d'onde, mais maintenant l'axe des y repr√©sente la fr√©quence en Hz.
L'intensit√© de la couleur donne l'amplitude ou la puissance de la composante fr√©quence √† chaque point dans le temps, mesur√©e en d√©cibels (dB).

Le spectrogramme est cr√©√© en prenant de courts segments du signal audio, g√©n√©ralement de quelques millisecondes, et en calculant la transform√©e de Fourier discr√®te de chaque segment pour obtenir son spectre de fr√©quences. 
Les spectres r√©sultants sont ensuite empil√©s sur l'axe temporel pour cr√©er le spectrogramme. 
Chaque tranche verticale de cette image correspond √† un spectre de fr√©quences unique, vu du haut. 
Par d√©faut, `librosa.stft()` divise le signal audio en segments de 2048 √©chantillons, ce qui donne un bon compromis entre la r√©solution de fr√©quence et la r√©solution temporelle.
√âtant donn√© que le spectrogramme et la forme d'onde sont des vues diff√©rentes des m√™mes donn√©es, il est possible de retourner le spectrogramme dans la forme d'onde d'origine en utilisant la TFCT inverse. 
Cependant, cela n√©cessite les informations de phase en plus des informations d'amplitude. Si le spectrogramme a √©t√© g√©n√©r√© par un mod√®le d'apprentissage automatique, il ne produit g√©n√©ralement que les amplitudes. 
Dans ce cas, nous pouvons utiliser un algorithme de reconstruction de phase tel que l'algorithme classique de Griffin-Lim, ou en utilisant un r√©seau neuronal appel√© vocodeur, pour reconstruire une forme d'onde √† partir du spectrogramme.

Les spectrogrammes ne sont pas seulement utilis√©s pour la visualisation. De nombreux mod√®les d'apprentissage automatique prendront des spectrogrammes en entr√©e, par opposition aux formes d'onde, et produiront des spectrogrammes en sortie.
Maintenant que nous savons ce qu'est un spectrogramme et comment il est fabriqu√©, jetons un coup d'≈ìil √† une variante de celui-ci largement utilis√©e pour le traitement de la parole: le spectrogramme mel.

## Spectrogramme Mel

Un spectrogramme mel est une variante du spectrogramme couramment utilis√©e dans les t√¢ches de traitement de la parole et d'apprentissage automatique.
Il est similaire √† un spectrogramme en ce sens qu'il montre le contenu en fr√©quence d'un signal audio au fil du temps, mais sur un axe de fr√©quence diff√©rent.
Dans un spectrogramme standard, l'axe de fr√©quence est lin√©aire et est mesur√© en hertz (Hz). 
Cependant, le syst√®me auditif humain est plus sensible aux changements dans les basses fr√©quences que dans les fr√©quences plus √©lev√©es, et cette sensibilit√© diminue logarithmiquement √† mesure que la fr√©quence augmente. 
L'√©chelle mel est une √©chelle perceptuelle qui se rapproche de la r√©ponse en fr√©quence non lin√©aire de l'oreille humaine.
Pour cr√©er un spectrogramme mel, le STFT est utilis√© comme auparavant, divisant l'audio en segments courts pour obtenir une s√©quence de spectres de fr√©quence. 
De plus, chaque spectre est envoy√© √† travers un ensemble de filtres, appel√© *mel filterbank*, pour transformer les fr√©quences √† l'√©chelle mel.
Voyons comment nous pouvons tracer un spectrogramme mel en utilisant la fonction `melspectrogram()` de librosa, qui effectue toutes ces √©tapes pour nous:

```py
S = librosa.feature.melspectrogram(y=array, sr=sampling_rate, n_mels=128, fmax=8000)
S_dB = librosa.power_to_db(S, ref=np.max)

plt.figure().set_figwidth(12)
librosa.display.specshow(S_dB, x_axis="time", y_axis="mel", sr=sampling_rate, fmax=8000)
plt.colorbar()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/mel-spectrogram.png" alt="Mel spectrogram plot">
</div>

Dans l'exemple ci-dessus, `n_mels` repr√©sente le nombre de bandes mel √† g√©n√©rer. 
Les bandes mel d√©finissent un ensemble de gammes de fr√©quences qui divisent le spectre en composantes perceptuellement significatives, en utilisant un ensemble de filtres dont la forme et l'espacement sont choisis pour imiter la fa√ßon dont l'oreille humaine r√©pond √† diff√©rentes fr√©quences.
Les valeurs courantes pour `n_mels` sont 40 ou 80. `fmax` indique la fr√©quence la plus √©lev√©e (en Hz) qui nous int√©resse.
Tout comme avec un spectrogramme standard, il est courant d'exprimer la force des composantes de fr√©quence mel en d√©cibels. 
C'est ce qu'on appelle commun√©ment un **spectrogramme log-mel**, car la conversion en d√©cibels implique une op√©ration logarithmique. 
L'exemple ci-dessus utilis√© `librosa.power_to_db()` car `librosa.feature.melspectrogram()` cr√©e un spectrogramme de puissance.

<Tip>
üí° Tous les spectrogrammes mel ne sont pas identiques ! Il existe deux √©chelles mel diff√©rentes d'usage courant (¬´ htk ¬ª et ¬´ slaney ¬ª), et au lieu du spectrogramme de puissance, le spectrogramme d'amplitude peut √™tre utilis√©. 
La conversion en spectrogramme log-mel ne calcule pas toujours les d√©cibels vrais, mais peut simplement prendre le ¬´ log ¬ª. 
Par cons√©quent, si un mod√®le d'apprentissage automatique attend un spectrogramme mel en entr√©e, v√©rifiez deux fois pour vous assurer que vous le calculez de la m√™me mani√®re.
</Tip>

La cr√©ation d'un spectrogramme mel est une op√©ration avec perte car elle implique le filtrage du signal. 
La conversion d'un spectrogramme mel en une forme d'onde est plus difficile que de le faire pour un spectrogramme r√©gulier, car cela n√©cessite d'estimer les fr√©quences qui ont √©t√© jet√©es. 
C'est pourquoi des mod√®les d'apprentissage automatique tels que le vocodeur HiFiGAN sont n√©cessaires pour produire une forme d'onde √† partir d'un spectrogramme mel.
Compar√© √† un spectrogramme standard, un spectrogramme mel peut capturer des caract√©ristiques plus significatives du signal audio pour la perception humaine, ce qui en fait un choix populaire dans des t√¢ches telles que la reconnaissance vocale, l'identification du locuteur et la classification des genres musicaux.
Maintenant que vous savez comment visualiser des exemples de donn√©es audio, essayez de voir √† quoi ressemblent vos sons pr√©f√©r√©s :)
