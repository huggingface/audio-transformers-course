# Ses veri kÃ¼mesini Ã¶n iÅŸleme

ğŸ¤— Veri KÃ¼meleri ile bir veri kÃ¼mesi yÃ¼klemek eÄŸlencenin sadece yarÄ±sÄ±dÄ±r. Bir modeli eÄŸitmek veya Ã§alÄ±ÅŸtÄ±rmak iÃ§in kullanmayÄ± planlÄ±yorsanÄ±z
Ã‡Ä±karÄ±m yapmak iÃ§in Ã¶ncelikle verileri Ã¶nceden iÅŸlemeniz gerekecektir. Genel olarak bu, aÅŸaÄŸÄ±daki adÄ±mlarÄ± iÃ§erecektir:

* Ses verilerinin yeniden Ã¶rneklenmesi
* Veri kÃ¼mesini filtreleme
* Ses verilerini modelin beklenen giriÅŸine dÃ¶nÃ¼ÅŸtÃ¼rme

## Ses verilerini yeniden Ã¶rnekleme


load_dataset iÅŸlevi, ses Ã¶rneklerini indirirken, bu Ã¶rneklerin yayÄ±nlandÄ±ÄŸÄ± Ã¶rnekleme hÄ±zÄ±nÄ± kullanÄ±r. Bu, eÄŸitmeyi veya Ã§Ä±karmayÄ± planladÄ±ÄŸÄ±nÄ±z modelin beklediÄŸi Ã¶rnekleme hÄ±zÄ± ile her zaman uyuÅŸmaz. Ã–rnekleme hÄ±zlarÄ± arasÄ±nda bir uyumsuzluk varsa, sesi modelin beklediÄŸi Ã¶rnekleme hÄ±zÄ±na yeniden Ã¶rnekleyebilirsiniz.


Ã‡oÄŸu Ã¶nceden eÄŸitilmiÅŸ model, ses veri kÃ¼melerinde 16 kHz Ã¶rnekleme hÄ±zÄ±nda Ã¶nceden eÄŸitildi. MINDS-14 veri kÃ¼mesini keÅŸfettiÄŸinizde, bu verinin 8 kHz Ã¶rnekleme hÄ±zÄ±nda olduÄŸunu gÃ¶rmÃ¼ÅŸ olabilirsiniz, bu da muhtemelen bunu yÃ¼kseltmemiz gerekeceÄŸi anlamÄ±na gelir.


Bunu yapmak iÃ§in, ğŸ¤— Datasets'Ä±n cast_column yÃ¶ntemini kullanabilirsiniz. Bu iÅŸlem sesi yerinde deÄŸiÅŸtirmez, ancak veri kÃ¼mesine ses Ã¶rneklerini yÃ¼klerken on-the-fly olarak Ã¶rnekleme yapmasÄ± gerektiÄŸini belirtir. AÅŸaÄŸÄ±daki kod, Ã¶rnekleme hÄ±zÄ±nÄ± 16 kHz olarak ayarlayacaktÄ±r:

```py
from datasets import Audio

minds = minds.cast_column("audio", Audio(sampling_rate=16_000))
```

MINDS-14 veri kÃ¼mesindeki ilk ses Ã¶rneÄŸini yeniden yÃ¼kleyin ve istenen "Ã¶rnekleme hÄ±zÄ±na" yeniden Ã¶rneklendiÄŸini kontrol edin:


```py
minds[0]
```

**Ã‡Ä±ktÄ±:**
```out
{
    "path": "/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-AU~PAY_BILL/response_4.wav",
    "audio": {
        "path": "/root/.cache/huggingface/datasets/downloads/extracted/f14948e0e84be638dd7943ac36518a4cf3324e8b7aa331c5ab11541518e9368c/en-AU~PAY_BILL/response_4.wav",
        "array": array(
            [
                2.0634243e-05,
                1.9437837e-04,
                2.2419340e-04,
                ...,
                9.3852862e-04,
                1.1302452e-03,
                7.1531429e-04,
            ],
            dtype=float32,
        ),
        "sampling_rate": 16000,
    },
    "transcription": "I would like to pay my electricity bill using my card can you please assist",
    "intent_class": 13,
}
```

Dizi deÄŸerlerinin de artÄ±k farklÄ± olduÄŸunu fark edebilirsiniz. Bunun nedeni artÄ±k iki kat daha fazla genlik deÄŸerine sahip olmamÄ±zdÄ±r.
daha Ã¶nce sahip olduÄŸumuz her biri.

<Tip>
ğŸ’¡ Ã–rnekleme hakkÄ±nda bazÄ± temel bilgiler: Bir ses sinyali 8 kHz'de Ã¶rneklenmiÅŸse, yani saniyede 8000 Ã¶rnek okuma alÄ±yorsak, sesin 4 kHz'nin Ã¼zerinde herhangi bir frekanseyi iÃ§ermediÄŸini biliyoruz. Bu, Nyquist Ã¶rnekleme teoremi tarafÄ±ndan garanti edilir. Bu nedenle, Ã¶rnekleme noktalarÄ± arasÄ±nda, orijinal sÃ¼rekli sinyalin her zaman dÃ¼zgÃ¼n bir eÄŸri oluÅŸturduÄŸundan emin olabiliriz. Ã–rnekleme hÄ±zÄ±nÄ± daha yÃ¼ksek bir Ã¶rnekleme hÄ±zÄ±na yÃ¼kseltmek, bu eÄŸriyi yaklaÅŸÄ±klayarak mevcut olanlarÄ±n arasÄ±na ek Ã¶rnek deÄŸerlerini hesaplamakla ilgilidir. Bununla birlikte, Ã¶rnekleme hÄ±zÄ±nÄ± dÃ¼ÅŸÃ¼rmek, Ã¶nce yeni Nyquist sÄ±nÄ±rÄ±ndan daha yÃ¼ksek frekanstaki herhangi bir frekansÄ± filtrelemeyi gerektirir, ardÄ±ndan yeni Ã¶rnek noktalarÄ±nÄ± tahmin etmeden Ã¶nce. Yani, Ã¶rnekleme hÄ±zÄ±nÄ± 2x faktÃ¶rÃ¼yle sadece her diÄŸer Ã¶rneÄŸi atarak dÃ¼ÅŸÃ¼remezsiniz - bu, sinyalde bozulmalara yol aÃ§an aliaslar adÄ± verilen bozulmalarÄ± oluÅŸturur. Ã–rnekleme iÅŸlemini doÄŸru bir ÅŸekilde yapmak karmaÅŸÄ±ktÄ±r ve en iyi test edilmiÅŸ kÃ¼tÃ¼phanelere, Ã¶rneÄŸin librosa veya ğŸ¤— Datasets gibi kÃ¼tÃ¼phanelere bÄ±rakÄ±lmalÄ±dÄ±r.
</Tip>

## Veri kÃ¼mesini filtreleme

Verileri bazÄ± kriterlere gÃ¶re filtrelemeniz gerekebilir. YaygÄ±n durumlardan biri, ses Ã¶rneklerinin belirli bir
Belirli sÃ¼re. Ã–rneÄŸin, yetersiz bellek hatalarÄ±nÄ± Ã¶nlemek iÃ§in 20 saniyeden uzun Ã¶rnekleri filtrelemek isteyebiliriz.
Bir modeli eÄŸitirken.


Bunu ğŸ¤— Datasets'in `filter` metodunu kullanarak ve ona filtreleme mantÄ±ÄŸÄ± olan bir fonksiyon geÃ§irerek yapabiliriz. Bir tane yazarak baÅŸlayalÄ±m
Hangi Ã¶rneklerin saklanacaÄŸÄ±nÄ± ve hangilerinin atÄ±lacaÄŸÄ±nÄ± gÃ¶steren iÅŸlev. Bu iÅŸlev, "ses_uzunluÄŸu_aralÄ±ÄŸÄ±ndadÄ±r",
Ã–rnek 20 saniyeden kÄ±saysa "DoÄŸru"yu, 20 saniyeden uzunsa "YanlÄ±ÅŸ"Ä± dÃ¶ndÃ¼rÃ¼r.

```py
MAX_DURATION_IN_SECONDS = 20.0


def is_audio_length_in_range(input_length):
    return input_length < MAX_DURATION_IN_SECONDS
```

Filtreleme iÅŸlevi bir veri kÃ¼mesinin sÃ¼tununa uygulanabilir ancak bunda ses izleme sÃ¼resine sahip bir sÃ¼tunumuz yoktur.
veri kÃ¼mesi. Ancak bir tane oluÅŸturabilir, o sÃ¼tundaki deÄŸerlere gÃ¶re filtreleyebilir ve ardÄ±ndan kaldÄ±rabiliriz.

```py
# ses dosyasÄ±ndan Ã¶rneÄŸin sÃ¼resini almak iÃ§in librosa'yÄ± kullanalÄ±m
new_column = [librosa.get_duration(path=x) for x in minds["path"]]
minds = minds.add_column("duration", new_column)

# filtreleme iÅŸlevini uygulamak iÃ§in ğŸ¤— Veri KÃ¼melerinin "filtre" yÃ¶ntemini kullanÄ±n
minds = minds.filter(is_audio_length_in_range, input_columns=["duration"])

# geÃ§ici yardÄ±mcÄ± sÃ¼tunu kaldÄ±rÄ±n
minds = minds.remove_columns(["duration"])
minds
```

**Ã‡Ä±ktÄ±:**
```out
Dataset({features: ["path", "audio", "transcription", "intent_class"], num_rows: 624})
```

Veri kÃ¼mesinin 654 Ã¶rnekten 624'e filtrelendiÄŸini doÄŸrulayabiliriz.

## Ses verilerinin Ã¶n iÅŸlenmesi

Ses veri kÃ¼meleriyle Ã§alÄ±ÅŸmanÄ±n en zorlayÄ±cÄ± yÃ¶nlerinden biri, veriyi model eÄŸitimi iÃ§in doÄŸru formatta hazÄ±rlamaktÄ±r. GÃ¶rdÃ¼ÄŸÃ¼nÃ¼z gibi, ham ses verileri Ã¶rnek deÄŸerlerinin bir dizisi olarak gelir. Ancak, kullanÄ±yorsanÄ±z, Ã§Ä±karÄ±m iÃ§in kullanÄ±yor olsanÄ±z veya gÃ¶reviniz iÃ§in ince ayar yapmak istiyorsanÄ±z, Ã¶nceden eÄŸitilmiÅŸ modeller, ham verilerin giriÅŸ Ã¶zelliklerine dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesini bekler. GiriÅŸ Ã¶zellikleri iÃ§in gereksinimler bir modelden diÄŸerine deÄŸiÅŸebilir - bunlar modelin mimarisine ve Ã¶nceden eÄŸitildiÄŸi verilere baÄŸlÄ±dÄ±r. Ä°yi haber ÅŸu ki, her desteklenen ses modeli iÃ§in ğŸ¤— Transformers, ham ses verilerini modelin beklediÄŸi giriÅŸ Ã¶zelliklerine dÃ¶nÃ¼ÅŸtÃ¼rebilen bir Ã¶zellik Ã§Ä±karÄ±cÄ± sÄ±nÄ±fÄ± sunar.

Peki Ã¶zellik Ã§Ä±karÄ±cÄ± ham ses verileriyle ne yapar? Åimdi [Whisper](https://huggingface.co/papers/2212.04356)'Ä±n fotoÄŸraflarÄ±na bir gÃ¶z atalÄ±m
BazÄ± ortak Ã¶zellik Ã§Ä±karma dÃ¶nÃ¼ÅŸÃ¼mlerini anlamak iÃ§in Ã¶zellik Ã§Ä±karÄ±cÄ±. Whisper, Ã¶nceden eÄŸitilmiÅŸ bir modeldir.
Alec Radford ve diÄŸerleri tarafÄ±ndan EylÃ¼l 2022'de yayÄ±nlanan otomatik konuÅŸma tanÄ±ma (ASR). OpenAI'den.

Ä°lk olarak, Whisper Ã¶zellik Ã§Ä±karÄ±cÄ±sÄ± bir ses Ã¶rneklerinin yÄ±ÄŸÄ±nÄ±nÄ±, tÃ¼m Ã¶rneklerin 30 saniyelik bir giriÅŸ uzunluÄŸuna sahip olduÄŸu ÅŸekilde doldurur/keser. Bu sÃ¼reden daha kÄ±sa olan Ã¶rnekler, sÄ±fÄ±rlarÄ± dizinin sonuna ekleyerek 30 saniyeye kadar doldurulur (bir ses sinyalindeki sÄ±fÄ±rlar, hiÃ§ sinyal veya sessizlikle karÅŸÄ±lÄ±k gelir). 30 saniyeden daha uzun olan Ã¶rnekler 30 saniyeye kadar kesilir. YÄ±ÄŸÄ±ndaki tÃ¼m Ã¶ÄŸeler giriÅŸ uzayÄ±ndaki maksimum uzunluÄŸa doldurulduÄŸu/kesildiÄŸi iÃ§in bir dikkat maskesine ihtiyaÃ§ yoktur. Whisper bu aÃ§Ä±dan benzersizdir, diÄŸer Ã§oÄŸu ses modelleri, dizilerin nerede doldurulduÄŸunu ayrÄ±ntÄ±lÄ± olarak belirten ve bu nedenle Ã¶z-dikkat mekanizmasÄ±nda nerede gÃ¶rmezden gelinmesi gerektiÄŸini belirten bir dikkat maskesi gerektirir. Whisper, bir dikkat maskesi olmadan Ã§alÄ±ÅŸacak ÅŸekilde eÄŸitilmiÅŸtir ve giriÅŸleri nerede gÃ¶z ardÄ± edeceÄŸini doÄŸrudan konuÅŸma sinyallerinden Ã§Ä±karÄ±r.

Whisper Ã¶zellik Ã§Ä±karÄ±cÄ±sÄ±nÄ±n gerÃ§ekleÅŸtirdiÄŸi ikinci iÅŸlem, doldurulmuÅŸ ses dizilerini log-mel spektrogramlarÄ±na dÃ¶nÃ¼ÅŸtÃ¼rmektir. HatÄ±rlarsanÄ±z, bu spektrogramlar bir sinyalin frekanslarÄ±nÄ±n zaman iÃ§inde nasÄ±l deÄŸiÅŸtiÄŸini, mel Ã¶lÃ§eÄŸinde ifade edilmiÅŸ ve insan iÅŸitmesini daha iyi temsil etmek iÃ§in decibel cinsinden Ã¶lÃ§Ã¼lmÃ¼ÅŸ ÅŸekilde aÃ§Ä±klar (log kÄ±smÄ±).

TÃ¼m bu dÃ¶nÃ¼ÅŸÃ¼mler birkaÃ§ satÄ±r kodla ham ses verilerinize uygulanabilir. Devam edelim ve yÃ¼kleyelim
ses verilerimize hazÄ±r olmak iÃ§in Ã¶nceden eÄŸitilmiÅŸ Whisper kontrol noktasÄ±ndan Ã¶zellik Ã§Ä±karÄ±cÄ±:

```py
from transformers import WhisperFeatureExtractor

feature_extractor = WhisperFeatureExtractor.from_pretrained("openai/whisper-small")
```

Daha sonra, tek bir ses Ã¶rneÄŸini "feature_extractor"dan geÃ§irerek Ã¶n iÅŸleme tabi tutacak bir iÅŸlev yazabilirsiniz.

```py
def prepare_dataset(example):
    audio = example["audio"]
    features = feature_extractor(
        audio["array"], sampling_rate=audio["sampling_rate"], padding=True
    )
    return features
```

Veri hazÄ±rlama fonksiyonunu tÃ¼m eÄŸitim Ã¶rneklerimize ğŸ¤— Datasets'in harita yÃ¶ntemini kullanarak uygulayabiliriz:

```py
minds = minds.map(prepare_dataset)
minds
```

**Ã‡Ä±ktÄ±:**
```out
Dataset(
    {
        features: ["path", "audio", "transcription", "intent_class", "input_features"],
        num_rows: 624,
    }
)
```

Bu kadar kolay, artÄ±k veri kÃ¼mesinde 'giriÅŸ_Ã¶zellikleri' olarak log-mel spektrogramlarÄ±mÄ±z var.

Bunu 'minds' veri kÃ¼mesindeki Ã¶rneklerden biri iÃ§in gÃ¶rselleÅŸtirelim:

```py
import numpy as np

example = minds[0]
input_features = example["input_features"]

plt.figure().set_figwidth(12)
librosa.display.specshow(
    np.asarray(input_features[0]),
    x_axis="time",
    y_axis="mel",
    sr=feature_extractor.sampling_rate,
    hop_length=feature_extractor.hop_length,
)
plt.colorbar()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/huggingface-course/audio-course-images/resolve/main/log_mel_whisper.png" alt="Log mel spectrogram plot">
</div>

ArtÄ±k Whisper modelindeki ses giriÅŸinin Ã¶n iÅŸlemeden sonra nasÄ±l gÃ¶rÃ¼ndÃ¼ÄŸÃ¼nÃ¼ gÃ¶rebilirsiniz.

Modelin Ã¶zellik Ã§Ä±karÄ±cÄ± sÄ±nÄ±fÄ±, ham ses verilerinin modelin beklediÄŸi formata dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesiyle ilgilenir. Fakat,
ses iÃ§eren birÃ§ok gÃ¶rev Ã§ok modludur; KonuÅŸma tanÄ±ma. Bu gibi durumlarda ğŸ¤— Transformers modele Ã¶zel olarak da hizmet vermektedir.
metin giriÅŸlerini iÅŸlemek iÃ§in belirteÃ§ler. Tokenizer'lara iliÅŸkin derinlemesine bilgi edinmek iÃ§in lÃ¼tfen [NLP kursumuza](https://huggingface.co/course/chapter2/4) bakÄ±n.

Whisper ve diÄŸer multimodal modeller iÃ§in Ã¶zellik Ã§Ä±karÄ±cÄ±yÄ± ve tokenizer'Ä± ayrÄ± ayrÄ± yÃ¼kleyebilir veya her ikisini de
sÃ¶zde iÅŸlemci. Ä°ÅŸleri daha da kolaylaÅŸtÄ±rmak iÃ§in, bir modelin Ã¶zellik Ã§Ä±karÄ±cÄ±sÄ±nÄ± ve iÅŸlemcisini bir bilgisayardan yÃ¼klemek iÃ§in 'Otomatik Ä°ÅŸlemci'yi kullanÄ±n.
kontrol noktasÄ± ÅŸÃ¶yle:

```py
from transformers import AutoProcessor

processor = AutoProcessor.from_pretrained("openai/whisper-small")
```

Burada temel veri hazÄ±rlama adÄ±mlarÄ±nÄ± gÃ¶sterdik. Elbette Ã¶zel veriler daha karmaÅŸÄ±k Ã¶n iÅŸleme gerektirebilir.
Bu durumda, herhangi bir Ã¶zel veri dÃ¶nÃ¼ÅŸÃ¼mÃ¼ gerÃ§ekleÅŸtirmek iÃ§in `prepare_dataset` fonksiyonunu geniÅŸletebilirsiniz. ğŸ¤— Veri KÃ¼meleri ile,
bunu bir Python iÅŸlevi olarak yazabiliyorsanÄ±z veri kÃ¼menize [uygulayabilirsiniz](https://huggingface.co/docs/datasets/audio_process)!!
